-- Logs begin at Wed 2018-08-22 17:32:50 UTC, end at Mon 2018-08-27 23:32:08 UTC. --
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.133436   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.296117   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.296180   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.308297   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Content-Type:[text/plain; charset=utf-8] Content-Length:[2] Date:[Mon, 27 Aug 2018 23:30:44 GMT]] 0xc420ed0c00 2 [] false false map[] 0xc420b9db00 0xc422c56420}
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.308386   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.408399   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.497273   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:30:33.143324355 +0000 UTC m=+1213.550833604
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.497326   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528772Ki, capacity: 40593708Ki, time: 2018-08-27 23:30:33.143324355 +0000 UTC m=+1213.550833604
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.497347   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:30:33.143324355 +0000 UTC m=+1213.550833604
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.497364   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61125376Ki, capacity: 61847136Ki
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.497379   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49446464Ki, capacity: 61847136Ki, time: 2018-08-27 23:30:33.143324355 +0000 UTC m=+1213.550833604
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.497395   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528772Ki, capacity: 40593708Ki, time: 2018-08-27 23:30:33.143324355 +0000 UTC m=+1213.550833604
Aug 27 23:30:44 swatig-vm kubelet[11716]: I0827 23:30:44.497429   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.133511   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.133609   11716 kubelet_pods.go:1382] Generating status for "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.134038   11716 status_manager.go:353] Ignoring same status for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:28:59 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [nvidia-dcgm-exporter]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:05 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 23:12:04 +0000 UTC InitContainerStatuses:[{Name:nvidia-dcgm-exporter-hook State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:12:06 +0000 UTC,FinishedAt:2018-08-27 23:12:06 +0000 UTC,ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e}] ContainerStatuses:[{Name:node-exporter State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 23:12:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/prometheus/node-exporter:v0.15.2 ImageID:docker-pullable://quay.io/prometheus/node-exporter@sha256:0c7dd2350bed76fce17dff8bd2a2ac599bc989c7328eb77b0751b8024cf0457d ContainerID:docker://76b7f83d228f08c90324183faf81862e43af14282064503023d73c07af58b035} {Name:nvidia-dcgm-exporter State:{Waiting:&ContainerStateWaiting{Reason:CrashLoo
Aug 27 23:30:45 swatig-vm kubelet[11716]: pBackOff,Message:Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:28:53 +0000 UTC,FinishedAt:2018-08-27 23:28:58 +0000 UTC,ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840,}} Ready:false RestartCount:8 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840}] QOSClass:Burstable}
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.134707   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.220838   11716 secret.go:186] Setting up volume default-token-dbt8n for pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.223167   11716 secret.go:216] Received secret default/default-token-dbt8n containing (3) pieces of data, 1878 total bytes
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.223551   11716 atomic_writer.go:156] pod default/node-exporter-node-8z55w volume default-token-dbt8n: no update required for target directory /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.435117   11716 volume_manager.go:372] All volumes are attached and mounted for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.435396   11716 kuberuntime_manager.go:514] Container {Name:nvidia-dcgm-exporter Image:containerdevk8s/dcgm-exporter:1.4.3 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:collector-textfiles ReadOnly:false MountPath:/run/prometheus SubPath: MountPropagation:<nil>} {Name:default-token-dbt8n ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false ExtendedResourceRequests:[] ComputeResourceRequests:[]} is dead, but RestartPolicy says that we should restart it.
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.435425   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e4457e539de698350ee03306991d2d3f618821e047b72516692b6353aa256e7f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[1] ContainersToKill:map[]} for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.435606   11716 kuberuntime_manager.go:758] checking backoff for container "nvidia-dcgm-exporter" in pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.435761   11716 kuberuntime_manager.go:768] Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.435786   11716 kuberuntime_manager.go:721] Backing Off restarting container &Container{Name:nvidia-dcgm-exporter,Image:containerdevk8s/dcgm-exporter:1.4.3,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[{collector-textfiles false /run/prometheus  <nil>} {default-token-dbt8n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,VolumeDevices:[],ExtendedResourceRequests:[],ComputeResourceRequests:[],} in pod node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:30:45 swatig-vm kubelet[11716]: E0827 23:30:45.435889   11716 pod_workers.go:186] Error syncing pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 ("node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"), skipping: failed to "StartContainer" for "nvidia-dcgm-exporter" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:45 swatig-vm kubelet[11716]: I0827 23:30:45.435924   11716 server.go:231] Event(v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"node-exporter-node-8z55w", UID:"9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991", APIVersion:"v1", ResourceVersion:"14310", FieldPath:"spec.containers{nvidia-dcgm-exporter}", ExtendedResources:v1.ExtendedResourceBinding(nil)}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
Aug 27 23:30:46 swatig-vm kubelet[11716]: I0827 23:30:46.133494   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)
Aug 27 23:30:46 swatig-vm kubelet[11716]: I0827 23:30:46.133606   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:46 swatig-vm kubelet[11716]: I0827 23:30:46.133691   11716 kubelet_pods.go:1382] Generating status for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:30:46 swatig-vm kubelet[11716]: I0827 23:30:46.133938   11716 status_manager.go:353] Ignoring same status for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:10:23 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:12 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:13 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/etcd-amd64:3.1.11 ImageID:docker-pullable://containerdevk8s/etcd-amd64@sha256:cc14b8e827085f97117e118618ef817b562b91abe1ca8e363357fe68f68311e3 ContainerID:docker://18a9649a6e568bff7f6ab37928f4d563623fe180a2ff0eb860aaa391f4332ef3}] QOSClass:BestEffort}
Aug 27 23:30:46 swatig-vm kubelet[11716]: I0827 23:30:46.134307   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:30:46 swatig-vm kubelet[11716]: I0827 23:30:46.434697   11716 volume_manager.go:372] All volumes are attached and mounted for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:30:46 swatig-vm kubelet[11716]: I0827 23:30:46.434866   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:30:47 swatig-vm kubelet[11716]: I0827 23:30:47.143755   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:30:47 swatig-vm kubelet[11716]: I0827 23:30:47.143826   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:47 swatig-vm kubelet[11716]: I0827 23:30:47.145114   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:30:47 GMT] Content-Length:[0]] 0xc421b264e0 0 [] true false map[] 0xc420fe3900 <nil>}
Aug 27 23:30:47 swatig-vm kubelet[11716]: I0827 23:30:47.145261   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:30:48 swatig-vm kubelet[11716]: I0827 23:30:48.133374   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:48 swatig-vm kubelet[11716]: I0827 23:30:48.586453   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:30:49 swatig-vm kubelet[11716]: I0827 23:30:49.357650   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:30:49 swatig-vm kubelet[11716]: I0827 23:30:49.357711   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:49 swatig-vm kubelet[11716]: I0827 23:30:49.359099   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:30:49 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc421b278e0 2 [] true false map[] 0xc420b9c500 <nil>}
Aug 27 23:30:49 swatig-vm kubelet[11716]: I0827 23:30:49.359184   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.133505   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.150652   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.150709   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.151911   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:30:50 GMT] Content-Length:[0]] 0xc421a61a80 0 [] true false map[] 0xc421f78200 <nil>}
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.151966   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.415897   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.415944   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.417077   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:30:50 GMT] Content-Length:[51]] 0xc420f547a0 51 [] true false map[] 0xc421f78800 <nil>}
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.417135   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.723050   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.723096   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.724059   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:30:50 GMT] Content-Length:[3] Content-Type:[text/plain; charset=utf-8]] 0xc421a61b40 3 [] true false map[] 0xc42144f700 <nil>}
Aug 27 23:30:50 swatig-vm kubelet[11716]: I0827 23:30:50.724131   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:30:51 swatig-vm kubelet[11716]: I0827 23:30:51.539866   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:30:51 swatig-vm kubelet[11716]: I0827 23:30:51.539944   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:51 swatig-vm kubelet[11716]: I0827 23:30:51.541766   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[18] Content-Type:[text/plain; charset=utf-8] Date:[Mon, 27 Aug 2018 23:30:51 GMT]] 0xc4209aa380 18 [] true false map[] 0xc42144fa00 <nil>}
Aug 27 23:30:51 swatig-vm kubelet[11716]: I0827 23:30:51.541852   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.133615   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.478752   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.478811   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.480116   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:30:52 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc4204b56e0 2 [] true false map[] 0xc421590700 <nil>}
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.480187   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.480571   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.480595   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.480982   11716 interface.go:174] Interface ens3 is up
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.481147   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.481180   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.481198   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.481213   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.481270   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.481312   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.885848   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.885911   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.895803   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:30:52 GMT]] 0xc4219a50a0 -1 [] true true map[] 0xc421590b00 <nil>}
Aug 27 23:30:52 swatig-vm kubelet[11716]: I0827 23:30:52.895892   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.018476   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.018536   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.019799   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[51] Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:30:53 GMT]] 0xc422a65a80 51 [] true false map[] 0xc421590d00 <nil>}
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.019861   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.133501   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.133606   11716 kubelet_pods.go:1382] Generating status for "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.133953   11716 status_manager.go:353] Ignoring same status for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:49 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:43 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:49 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/kube-proxy-amd64:v1.9.10 ImageID:docker-pullable://containerdevk8s/kube-proxy-amd64@sha256:ea3aef50e4b9d17616af8beb339da298c0ab71569c816c6628edfff4bdd0908a ContainerID:docker://d53fefdf7557fdbedcce62abacdab2016e6b98a0e122b306779fa04622158664}] QOSClass:BestEffort}
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.134382   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.151171   11716 configmap.go:187] Setting up volume kube-proxy for pod 8e41f9ca-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/kube-proxy
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.151228   11716 secret.go:186] Setting up volume kube-proxy-token-6hqvv for pod 8e41f9ca-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/kube-proxy-token-6hqvv
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.153611   11716 configmap.go:217] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1209 total bytes
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.153625   11716 secret.go:216] Received secret kube-system/kube-proxy-token-6hqvv containing (3) pieces of data, 1904 total bytes
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.153962   11716 atomic_writer.go:156] pod kube-system/kube-proxy-x6vpl volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/kube-proxy
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.154195   11716 atomic_writer.go:156] pod kube-system/kube-proxy-x6vpl volume kube-proxy-token-6hqvv: no update required for target directory /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/kube-proxy-token-6hqvv
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.434894   11716 volume_manager.go:372] All volumes are attached and mounted for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.435112   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:53 swatig-vm kubelet[11716]: I0827 23:30:53.588819   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.133530   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.296125   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.296167   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.308549   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Date:[Mon, 27 Aug 2018 23:30:54 GMT] Content-Type:[text/plain; charset=utf-8] Content-Length:[2]] 0xc420b74c00 2 [] false false map[] 0xc42046ee00 0xc420ae0580}
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.308631   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.497758   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.591410   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49446496Ki, capacity: 61847136Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.591467   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528732Ki, capacity: 40593708Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.591490   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.591510   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528732Ki, capacity: 40593708Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.591528   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.591545   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61125364Ki, capacity: 61847136Ki
Aug 27 23:30:54 swatig-vm kubelet[11716]: I0827 23:30:54.591576   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:30:56 swatig-vm kubelet[11716]: I0827 23:30:56.133526   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:57 swatig-vm kubelet[11716]: I0827 23:30:57.143776   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:30:57 swatig-vm kubelet[11716]: I0827 23:30:57.143845   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:57 swatig-vm kubelet[11716]: I0827 23:30:57.145152   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[0] Date:[Mon, 27 Aug 2018 23:30:57 GMT]] 0xc422a58220 0 [] true false map[] 0xc42098ef00 <nil>}
Aug 27 23:30:57 swatig-vm kubelet[11716]: I0827 23:30:57.145214   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.133603   11716 kubelet.go:1921] SyncLoop (SYNC): 2 pods; node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991), nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.134682   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.134768   11716 kubelet_pods.go:1382] Generating status for "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.134905   11716 kubelet_pods.go:1382] Generating status for "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.135981   11716 status_manager.go:353] Ignoring same status for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:28:59 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [nvidia-dcgm-exporter]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:05 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 23:12:04 +0000 UTC InitContainerStatuses:[{Name:nvidia-dcgm-exporter-hook State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:12:06 +0000 UTC,FinishedAt:2018-08-27 23:12:06 +0000 UTC,ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e}] ContainerStatuses:[{Name:node-exporter State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 23:12:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/prometheus/node-exporter:v0.15.2 ImageID:docker-pullable://quay.io/prometheus/node-exporter@sha256:0c7dd2350bed76fce17dff8bd2a2ac599bc989c7328eb77b0751b8024cf0457d ContainerID:docker://76b7f83d228f08c90324183faf81862e43af14282064503023d73c07af58b035} {Name:nvidia-dcgm-exporter State:{Waiting:&ContainerStateWaiting{Reason:CrashLoo
Aug 27 23:30:58 swatig-vm kubelet[11716]: pBackOff,Message:Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:28:53 +0000 UTC,FinishedAt:2018-08-27 23:28:58 +0000 UTC,ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840,}} Ready:false RestartCount:8 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840}] QOSClass:Burstable}
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.136587   11716 status_manager.go:353] Ignoring same status for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:08 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:10.244.0.4 StartTime:2018-08-27 20:12:56 +0000 UTC InitContainerStatuses:[{Name:nvidia-device-plugin-install-hooks State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 20:13:16 +0000 UTC,FinishedAt:2018-08-27 20:13:17 +0000 UTC,ContainerID:docker://7995e2f592845dadc2ab7b5fe764b95833f0b8fd967fc77ade4bbdcc25544f8e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/device-plugin:1.9.10 ImageID:docker-pullable://containerdevk8s/device-plugin@sha256:81de32888c89a1371ce585d2d35f71284b91d99deef7f34a963e2ba13a674b31 ContainerID:docker://7995e2f592845dadc2ab7b5fe764b95833f0b8fd967fc77ade4bbdcc25544f8e}] ContainerStatuses:[{Name:nvidia-device-plugin-ctr State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:13:18 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/device-plugin:1.9.10 ImageID:docker-pullable://containerdevk8s/device-plugin@sha256:81de32888c89a1371ce585d2d35f71284b91d99deef7f34a963e2ba13a674b31 ContainerID:docker://360f774547db49fddb48c28d2d83c4bb700d219f4e5494ec0b14f18f5142e166}] QOSClass:BestEffort}
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.136962   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.137074   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.175250   11716 secret.go:186] Setting up volume default-token-dbt8n for pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.175323   11716 secret.go:186] Setting up volume default-token-8flwv for pod 95ec07a4-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/95ec07a4-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-8flwv
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.177892   11716 secret.go:216] Received secret kube-system/default-token-8flwv containing (3) pieces of data, 1892 total bytes
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.178047   11716 secret.go:216] Received secret default/default-token-dbt8n containing (3) pieces of data, 1878 total bytes
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.178297   11716 atomic_writer.go:156] pod kube-system/nvidia-device-plugin-daemonset-6fbqg volume default-token-8flwv: no update required for target directory /var/lib/kubelet/pods/95ec07a4-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-8flwv
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.178427   11716 atomic_writer.go:156] pod default/node-exporter-node-8z55w volume default-token-dbt8n: no update required for target directory /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.437457   11716 volume_manager.go:372] All volumes are attached and mounted for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.437646   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.437482   11716 volume_manager.go:372] All volumes are attached and mounted for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.437939   11716 kuberuntime_manager.go:514] Container {Name:nvidia-dcgm-exporter Image:containerdevk8s/dcgm-exporter:1.4.3 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:collector-textfiles ReadOnly:false MountPath:/run/prometheus SubPath: MountPropagation:<nil>} {Name:default-token-dbt8n ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false ExtendedResourceRequests:[] ComputeResourceRequests:[]} is dead, but RestartPolicy says that we should restart it.
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.437984   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e4457e539de698350ee03306991d2d3f618821e047b72516692b6353aa256e7f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[1] ContainersToKill:map[]} for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.438213   11716 kuberuntime_manager.go:758] checking backoff for container "nvidia-dcgm-exporter" in pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.438396   11716 kuberuntime_manager.go:768] Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.438424   11716 kuberuntime_manager.go:721] Backing Off restarting container &Container{Name:nvidia-dcgm-exporter,Image:containerdevk8s/dcgm-exporter:1.4.3,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[{collector-textfiles false /run/prometheus  <nil>} {default-token-dbt8n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,VolumeDevices:[],ExtendedResourceRequests:[],ComputeResourceRequests:[],} in pod node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:30:58 swatig-vm kubelet[11716]: E0827 23:30:58.438548   11716 pod_workers.go:186] Error syncing pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 ("node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"), skipping: failed to "StartContainer" for "nvidia-dcgm-exporter" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.438590   11716 server.go:231] Event(v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"node-exporter-node-8z55w", UID:"9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991", APIVersion:"v1", ResourceVersion:"14310", FieldPath:"spec.containers{nvidia-dcgm-exporter}", ExtendedResources:v1.ExtendedResourceBinding(nil)}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
Aug 27 23:30:58 swatig-vm kubelet[11716]: I0827 23:30:58.591542   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:30:59 swatig-vm kubelet[11716]: I0827 23:30:59.357711   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:30:59 swatig-vm kubelet[11716]: I0827 23:30:59.357769   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:30:59 swatig-vm kubelet[11716]: I0827 23:30:59.360078   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:30:59 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc422825220 2 [] true false map[] 0xc42144f700 <nil>}
Aug 27 23:30:59 swatig-vm kubelet[11716]: I0827 23:30:59.360145   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.133658   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.150582   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.150643   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.151777   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:00 GMT] Content-Length:[0]] 0xc420fe5620 0 [] true false map[] 0xc420ba4100 <nil>}
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.151835   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.415950   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.416006   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.417417   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:00 GMT] Content-Length:[51]] 0xc420224de0 51 [] true false map[] 0xc420e92b00 <nil>}
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.417501   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.723054   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.723110   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.724398   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:00 GMT] Content-Length:[3] Content-Type:[text/plain; charset=utf-8]] 0xc420f46200 3 [] true false map[] 0xc420e92d00 <nil>}
Aug 27 23:31:00 swatig-vm kubelet[11716]: I0827 23:31:00.724468   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:01 swatig-vm kubelet[11716]: I0827 23:31:01.539698   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:31:01 swatig-vm kubelet[11716]: I0827 23:31:01.539740   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:01 swatig-vm kubelet[11716]: I0827 23:31:01.541200   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[18] Content-Type:[text/plain; charset=utf-8] Date:[Mon, 27 Aug 2018 23:31:01 GMT]] 0xc420f471e0 18 [] true false map[] 0xc420ba4d00 <nil>}
Aug 27 23:31:01 swatig-vm kubelet[11716]: I0827 23:31:01.541312   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.133576   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.478809   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.478860   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.480264   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:02 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc422834a20 2 [] true false map[] 0xc420ba5300 <nil>}
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.480339   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.497378   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.497414   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.497836   11716 interface.go:174] Interface ens3 is up
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.498075   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.498110   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.498126   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.498141   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.498155   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.498196   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.885790   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.885846   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.895866   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:31:02 GMT]] 0xc42292fb80 -1 [] true true map[] 0xc421f78d00 <nil>}
Aug 27 23:31:02 swatig-vm kubelet[11716]: I0827 23:31:02.895946   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:31:03 swatig-vm kubelet[11716]: I0827 23:31:03.018469   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:31:03 swatig-vm kubelet[11716]: I0827 23:31:03.018526   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:03 swatig-vm kubelet[11716]: I0827 23:31:03.019631   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:03 GMT] Content-Length:[51]] 0xc420e9d9c0 51 [] true false map[] 0xc420ba5900 <nil>}
Aug 27 23:31:03 swatig-vm kubelet[11716]: I0827 23:31:03.019704   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:31:03 swatig-vm kubelet[11716]: I0827 23:31:03.593578   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.133532   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.296129   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.296188   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.308050   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Content-Type:[text/plain; charset=utf-8] Content-Length:[2] Date:[Mon, 27 Aug 2018 23:31:04 GMT]] 0xc42121eb60 2 [] false false map[] 0xc420b9cb00 0xc421447130}
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.308118   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.591892   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.697588   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49446496Ki, capacity: 61847136Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.697660   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528732Ki, capacity: 40593708Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.697687   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.697710   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528732Ki, capacity: 40593708Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.697731   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:30:50.422593919 +0000 UTC m=+1230.830103154
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.697753   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61125000Ki, capacity: 61847136Ki
Aug 27 23:31:04 swatig-vm kubelet[11716]: I0827 23:31:04.697787   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:31:06 swatig-vm kubelet[11716]: I0827 23:31:06.133521   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:07 swatig-vm kubelet[11716]: I0827 23:31:07.143646   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:31:07 swatig-vm kubelet[11716]: I0827 23:31:07.143712   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:07 swatig-vm kubelet[11716]: I0827 23:31:07.145089   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:07 GMT] Content-Length:[0]] 0xc420b9aba0 0 [] true false map[] 0xc42144f200 <nil>}
Aug 27 23:31:07 swatig-vm kubelet[11716]: I0827 23:31:07.145150   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:08 swatig-vm kubelet[11716]: I0827 23:31:08.133429   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:08 swatig-vm kubelet[11716]: I0827 23:31:08.596057   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:09 swatig-vm kubelet[11716]: I0827 23:31:09.357722   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:31:09 swatig-vm kubelet[11716]: I0827 23:31:09.358569   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:09 swatig-vm kubelet[11716]: I0827 23:31:09.360666   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:09 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc420f47ae0 2 [] true false map[] 0xc42144fb00 <nil>}
Aug 27 23:31:09 swatig-vm kubelet[11716]: I0827 23:31:09.361876   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.133509   11716 kubelet.go:1921] SyncLoop (SYNC): 2 pods; node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991), tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.134368   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.134438   11716 kubelet_pods.go:1382] Generating status for "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.134589   11716 kubelet_pods.go:1382] Generating status for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.137156   11716 status_manager.go:353] Ignoring same status for pod "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:18:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:10:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:18:00 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:10.244.0.6 StartTime:2018-08-27 20:18:00 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:tiller State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:18:03 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:gcr.io/kubernetes-helm/tiller:v2.9.1 ImageID:docker-pullable://gcr.io/kubernetes-helm/tiller@sha256:417aae19a0709075df9cc87e2fcac599b39d8f73ac95e668d9627fec9d341af2 ContainerID:docker://ad71e4fdd5480b245c59f599a2ca145921ca2bc7d81ab02cab46ebc32187301c}] QOSClass:BestEffort}
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.139381   11716 status_manager.go:353] Ignoring same status for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:28:59 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [nvidia-dcgm-exporter]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:05 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 23:12:04 +0000 UTC InitContainerStatuses:[{Name:nvidia-dcgm-exporter-hook State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:12:06 +0000 UTC,FinishedAt:2018-08-27 23:12:06 +0000 UTC,ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e}] ContainerStatuses:[{Name:node-exporter State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 23:12:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/prometheus/node-exporter:v0.15.2 ImageID:docker-pullable://quay.io/prometheus/node-exporter@sha256:0c7dd2350bed76fce17dff8bd2a2ac599bc989c7328eb77b0751b8024cf0457d ContainerID:docker://76b7f83d228f08c90324183faf81862e43af14282064503023d73c07af58b035} {Name:nvidia-dcgm-exporter State:{Waiting:&ContainerStateWaiting{Reason:CrashLoo
Aug 27 23:31:10 swatig-vm kubelet[11716]: pBackOff,Message:Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:28:53 +0000 UTC,FinishedAt:2018-08-27 23:28:58 +0000 UTC,ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840,}} Ready:false RestartCount:8 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840}] QOSClass:Burstable}
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.142405   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.142528   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.150647   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.150709   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.152789   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:10 GMT] Content-Length:[0]] 0xc422249020 0 [] true false map[] 0xc421590b00 <nil>}
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.152863   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.219382   11716 secret.go:186] Setting up volume tiller-token-nzvkl for pod 4b593dd5-aa36-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/4b593dd5-aa36-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/tiller-token-nzvkl
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.219416   11716 secret.go:186] Setting up volume default-token-dbt8n for pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.222702   11716 secret.go:216] Received secret default/default-token-dbt8n containing (3) pieces of data, 1878 total bytes
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.223463   11716 atomic_writer.go:156] pod default/node-exporter-node-8z55w volume default-token-dbt8n: no update required for target directory /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.223547   11716 secret.go:216] Received secret kube-system/tiller-token-nzvkl containing (3) pieces of data, 1888 total bytes
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.223950   11716 atomic_writer.go:156] pod kube-system/tiller-deploy-79d7d4fddd-22q4w volume tiller-token-nzvkl: no update required for target directory /var/lib/kubelet/pods/4b593dd5-aa36-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/tiller-token-nzvkl
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.416060   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.416121   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.417582   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:10 GMT] Content-Length:[51]] 0xc4220904a0 51 [] true false map[] 0xc421f79b00 <nil>}
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.417669   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.443755   11716 volume_manager.go:372] All volumes are attached and mounted for pod "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.444001   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:efeaa58b46bbfef2205f44d0acf5b4bce51f4526c94b584cd76d9c95ac4e500f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.444521   11716 volume_manager.go:372] All volumes are attached and mounted for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.444768   11716 kuberuntime_manager.go:514] Container {Name:nvidia-dcgm-exporter Image:containerdevk8s/dcgm-exporter:1.4.3 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:collector-textfiles ReadOnly:false MountPath:/run/prometheus SubPath: MountPropagation:<nil>} {Name:default-token-dbt8n ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false ExtendedResourceRequests:[] ComputeResourceRequests:[]} is dead, but RestartPolicy says that we should restart it.
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.444800   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e4457e539de698350ee03306991d2d3f618821e047b72516692b6353aa256e7f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[1] ContainersToKill:map[]} for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.444983   11716 kuberuntime_manager.go:758] checking backoff for container "nvidia-dcgm-exporter" in pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.445187   11716 kuberuntime_manager.go:768] Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.445214   11716 kuberuntime_manager.go:721] Backing Off restarting container &Container{Name:nvidia-dcgm-exporter,Image:containerdevk8s/dcgm-exporter:1.4.3,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[{collector-textfiles false /run/prometheus  <nil>} {default-token-dbt8n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,VolumeDevices:[],ExtendedResourceRequests:[],ComputeResourceRequests:[],} in pod node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:10 swatig-vm kubelet[11716]: E0827 23:31:10.445390   11716 pod_workers.go:186] Error syncing pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 ("node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"), skipping: failed to "StartContainer" for "nvidia-dcgm-exporter" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.445403   11716 server.go:231] Event(v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"node-exporter-node-8z55w", UID:"9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991", APIVersion:"v1", ResourceVersion:"14310", FieldPath:"spec.containers{nvidia-dcgm-exporter}", ExtendedResources:v1.ExtendedResourceBinding(nil)}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.723016   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.723088   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.724346   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:10 GMT] Content-Length:[3] Content-Type:[text/plain; charset=utf-8]] 0xc4227b6460 3 [] true false map[] 0xc421590d00 <nil>}
Aug 27 23:31:10 swatig-vm kubelet[11716]: I0827 23:31:10.724413   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:11 swatig-vm kubelet[11716]: I0827 23:31:11.539773   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:31:11 swatig-vm kubelet[11716]: I0827 23:31:11.539833   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:11 swatig-vm kubelet[11716]: I0827 23:31:11.541878   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:11 GMT] Content-Length:[18] Content-Type:[text/plain; charset=utf-8]] 0xc422091d80 18 [] true false map[] 0xc421590f00 <nil>}
Aug 27 23:31:11 swatig-vm kubelet[11716]: I0827 23:31:11.541952   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.017933   11716 reflector.go:428] k8s.io/kubernetes/pkg/kubelet/kubelet.go:482: Watch close - *v1.Node total 38 items received
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.133576   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.478686   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.478734   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.479987   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:12 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc421a36fa0 2 [] true false map[] 0xc421591500 <nil>}
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.480063   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.512805   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.512847   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.513570   11716 interface.go:174] Interface ens3 is up
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.513752   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.513789   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.513810   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.513829   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.513847   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.513895   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.885839   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.885899   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.895702   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:31:12 GMT]] 0xc420fa4040 -1 [] true true map[] 0xc421591700 <nil>}
Aug 27 23:31:12 swatig-vm kubelet[11716]: I0827 23:31:12.895783   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:31:13 swatig-vm kubelet[11716]: I0827 23:31:13.018417   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:31:13 swatig-vm kubelet[11716]: I0827 23:31:13.018462   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:13 swatig-vm kubelet[11716]: I0827 23:31:13.019705   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[51] Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:13 GMT]] 0xc420fa4120 51 [] true false map[] 0xc421591900 <nil>}
Aug 27 23:31:13 swatig-vm kubelet[11716]: I0827 23:31:13.019771   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:31:13 swatig-vm kubelet[11716]: I0827 23:31:13.599256   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.133400   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.296197   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.296244   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.308669   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Content-Type:[text/plain; charset=utf-8] Content-Length:[2] Date:[Mon, 27 Aug 2018 23:31:14 GMT]] 0xc4228245c0 2 [] false false map[] 0xc422a5a300 0xc42185a580}
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.308755   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.698111   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.778125   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528696Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.778173   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.778191   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528696Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.778204   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.778219   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61125144Ki, capacity: 61847136Ki
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.778231   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49446264Ki, capacity: 61847136Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:14 swatig-vm kubelet[11716]: I0827 23:31:14.778257   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:31:16 swatig-vm kubelet[11716]: I0827 23:31:16.133545   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:17 swatig-vm kubelet[11716]: I0827 23:31:17.143759   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:31:17 swatig-vm kubelet[11716]: I0827 23:31:17.143818   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:17 swatig-vm kubelet[11716]: I0827 23:31:17.145272   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[0] Date:[Mon, 27 Aug 2018 23:31:17 GMT]] 0xc4223365e0 0 [] true false map[] 0xc4221b5000 <nil>}
Aug 27 23:31:17 swatig-vm kubelet[11716]: I0827 23:31:17.145357   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:18 swatig-vm kubelet[11716]: I0827 23:31:18.133515   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:18 swatig-vm kubelet[11716]: I0827 23:31:18.602105   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:19 swatig-vm kubelet[11716]: I0827 23:31:19.357759   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:31:19 swatig-vm kubelet[11716]: I0827 23:31:19.357822   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:19 swatig-vm kubelet[11716]: I0827 23:31:19.359257   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:19 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc420f79a20 2 [] true false map[] 0xc420b9d900 <nil>}
Aug 27 23:31:19 swatig-vm kubelet[11716]: I0827 23:31:19.359328   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.133503   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.148501   11716 qos_container_manager_linux.go:317] [ContainerManager]: Updated QoS cgroup configuration
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.150663   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.150726   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.152190   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:20 GMT] Content-Length:[0]] 0xc422835100 0 [] true false map[] 0xc420fe2700 <nil>}
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.152272   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.416021   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.416110   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.417609   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:20 GMT] Content-Length:[51]] 0xc420fe5d80 51 [] true false map[] 0xc420b9db00 <nil>}
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.417678   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.723147   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.723217   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.724538   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:20 GMT] Content-Length:[3] Content-Type:[text/plain; charset=utf-8]] 0xc420e9d060 3 [] true false map[] 0xc422816800 <nil>}
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.724596   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:20 swatig-vm kubelet[11716]: I0827 23:31:20.996554   11716 kubelet.go:1275] Container garbage collection succeeded
Aug 27 23:31:21 swatig-vm kubelet[11716]: I0827 23:31:21.539812   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:31:21 swatig-vm kubelet[11716]: I0827 23:31:21.539877   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:21 swatig-vm kubelet[11716]: I0827 23:31:21.542293   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:21 GMT] Content-Length:[18] Content-Type:[text/plain; charset=utf-8]] 0xc4225f6660 18 [] true false map[] 0xc420fe3800 <nil>}
Aug 27 23:31:21 swatig-vm kubelet[11716]: I0827 23:31:21.542404   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.133538   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.478780   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.478850   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.480324   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:22 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc422f18e00 2 [] true false map[] 0xc422a5a300 <nil>}
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.480402   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.530890   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.530933   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.531440   11716 interface.go:174] Interface ens3 is up
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.531610   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.531649   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.531670   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.531688   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.531705   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.531753   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.885753   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.885819   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.897434   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:31:22 GMT]] 0xc4219e6140 -1 [] true true map[] 0xc422a5ad00 <nil>}
Aug 27 23:31:22 swatig-vm kubelet[11716]: I0827 23:31:22.897522   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.018427   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.018494   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.019768   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:23 GMT] Content-Length:[51]] 0xc422631320 51 [] true false map[] 0xc421f78200 <nil>}
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.019859   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.137846   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.139700   11716 kubelet_pods.go:1382] Generating status for "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.140485   11716 status_manager.go:353] Ignoring same status for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:28:59 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [nvidia-dcgm-exporter]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:05 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 23:12:04 +0000 UTC InitContainerStatuses:[{Name:nvidia-dcgm-exporter-hook State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:12:06 +0000 UTC,FinishedAt:2018-08-27 23:12:06 +0000 UTC,ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e}] ContainerStatuses:[{Name:node-exporter State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 23:12:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/prometheus/node-exporter:v0.15.2 ImageID:docker-pullable://quay.io/prometheus/node-exporter@sha256:0c7dd2350bed76fce17dff8bd2a2ac599bc989c7328eb77b0751b8024cf0457d ContainerID:docker://76b7f83d228f08c90324183faf81862e43af14282064503023d73c07af58b035} {Name:nvidia-dcgm-exporter State:{Waiting:&ContainerStateWaiting{Reason:CrashLoo
Aug 27 23:31:23 swatig-vm kubelet[11716]: pBackOff,Message:Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:28:53 +0000 UTC,FinishedAt:2018-08-27 23:28:58 +0000 UTC,ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840,}} Ready:false RestartCount:8 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840}] QOSClass:Burstable}
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.141286   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.150820   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-a8a0b1b2a5d746181ed9d1a313d784acba7a80a69768207653d23cd8b836bcd3-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.151481   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-a8a0b1b2a5d746181ed9d1a313d784acba7a80a69768207653d23cd8b836bcd3-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.152110   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-a8a0b1b2a5d746181ed9d1a313d784acba7a80a69768207653d23cd8b836bcd3-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.152701   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-lxcfs.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.153357   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-lxcfs.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.153940   11716 manager.go:930] ignoring container "/system.slice/var-lib-lxcfs.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.154530   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-608ad7a244d8d8b1bc4c1e88e6d6921ae0af015887060b19be870b602e961d56-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.155125   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-608ad7a244d8d8b1bc4c1e88e6d6921ae0af015887060b19be870b602e961d56-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.155706   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-608ad7a244d8d8b1bc4c1e88e6d6921ae0af015887060b19be870b602e961d56-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.156304   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-29b24912\\x2daa4a\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2ddbt8n.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.156882   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-29b24912\\x2daa4a\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2ddbt8n.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.157523   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-29b24912\\x2daa4a\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2ddbt8n.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.158128   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-f16a5de14d3fcbf89a173fd567ef9b90c408011110d5e667a4e547d66e14b019-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.158716   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-f16a5de14d3fcbf89a173fd567ef9b90c408011110d5e667a4e547d66e14b019-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.159276   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-f16a5de14d3fcbf89a173fd567ef9b90c408011110d5e667a4e547d66e14b019-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.159805   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-0d3162f6893d788010883fd1407ae37262e31280dd4d45a823995dd8cdf0d2ef-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.160370   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-0d3162f6893d788010883fd1407ae37262e31280dd4d45a823995dd8cdf0d2ef-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.160917   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-0d3162f6893d788010883fd1407ae37262e31280dd4d45a823995dd8cdf0d2ef-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.161497   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-8e3a46b3\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-kube\\x2ddns\\x2dtoken\\x2ddmjrw.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.162049   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-8e3a46b3\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-kube\\x2ddns\\x2dtoken\\x2ddmjrw.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.162600   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-8e3a46b3\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-kube\\x2ddns\\x2dtoken\\x2ddmjrw.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163185   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163220   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163245   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163264   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-efeaa58b46bbfef2205f44d0acf5b4bce51f4526c94b584cd76d9c95ac4e500f-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163284   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-efeaa58b46bbfef2205f44d0acf5b4bce51f4526c94b584cd76d9c95ac4e500f-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163304   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-efeaa58b46bbfef2205f44d0acf5b4bce51f4526c94b584cd76d9c95ac4e500f-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163323   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-4f0b17ac18f30d938cf6a2649aca496d0938c6f150b85f4092193f303ac2b382-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163342   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-4f0b17ac18f30d938cf6a2649aca496d0938c6f150b85f4092193f303ac2b382-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163362   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-4f0b17ac18f30d938cf6a2649aca496d0938c6f150b85f4092193f303ac2b382-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163380   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-d4cc02b03a5d9a17b8437cbdadb11248239e18bbf8aa1218526eed7cc5139e96-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163399   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-d4cc02b03a5d9a17b8437cbdadb11248239e18bbf8aa1218526eed7cc5139e96-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163419   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-d4cc02b03a5d9a17b8437cbdadb11248239e18bbf8aa1218526eed7cc5139e96-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163438   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-8e425969\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-flannel\\x2dtoken\\x2ddww48.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163460   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-8e425969\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-flannel\\x2dtoken\\x2ddww48.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163482   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-8e425969\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-flannel\\x2dtoken\\x2ddww48.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163504   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-user-112.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163520   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/run-user-112.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163536   11716 manager.go:930] ignoring container "/system.slice/run-user-112.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163552   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163571   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163592   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163610   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-41c1e432a7c0d7045ed8c9af8bc2007187171305fa8eb8efaec35a921a9b786c-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163629   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-41c1e432a7c0d7045ed8c9af8bc2007187171305fa8eb8efaec35a921a9b786c-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163649   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-41c1e432a7c0d7045ed8c9af8bc2007187171305fa8eb8efaec35a921a9b786c-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163668   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-18a9649a6e568bff7f6ab37928f4d563623fe180a2ff0eb860aaa391f4332ef3-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163686   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-18a9649a6e568bff7f6ab37928f4d563623fe180a2ff0eb860aaa391f4332ef3-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163706   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-18a9649a6e568bff7f6ab37928f4d563623fe180a2ff0eb860aaa391f4332ef3-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163725   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-e8914906cafec6e62833d1b91dfa242518bbfad9aa803f5d54270b8a694c8c1f-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163743   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-e8914906cafec6e62833d1b91dfa242518bbfad9aa803f5d54270b8a694c8c1f-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163763   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-e8914906cafec6e62833d1b91dfa242518bbfad9aa803f5d54270b8a694c8c1f-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163781   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-4e1b3dcc3db5c9965066f13f37a372904bf5ebf833a5c57eeca459afb1ee833f-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163800   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-4e1b3dcc3db5c9965066f13f37a372904bf5ebf833a5c57eeca459afb1ee833f-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163819   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-4e1b3dcc3db5c9965066f13f37a372904bf5ebf833a5c57eeca459afb1ee833f-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163837   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-67b5fdedb7163aefa4e01bd07760686a7a47ba80b586703dec2b185f91a28f04-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163856   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-67b5fdedb7163aefa4e01bd07760686a7a47ba80b586703dec2b185f91a28f04-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163876   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-67b5fdedb7163aefa4e01bd07760686a7a47ba80b586703dec2b185f91a28f04-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163894   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-a4e6f975\\x2daa36\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-prometheus\\x2doperator\\x2dtoken\\x2dr2g9q.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163916   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-a4e6f975\\x2daa36\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-prometheus\\x2doperator\\x2dtoken\\x2dr2g9q.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163945   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-a4e6f975\\x2daa36\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-prometheus\\x2doperator\\x2dtoken\\x2dr2g9q.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163968   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-4ebb3f9dbc89b8ca0e9365c7d3466d4c3e89af3988f90cc48ae14f20435a88d3-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.163987   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-4ebb3f9dbc89b8ca0e9365c7d3466d4c3e89af3988f90cc48ae14f20435a88d3-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164007   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-4ebb3f9dbc89b8ca0e9365c7d3466d4c3e89af3988f90cc48ae14f20435a88d3-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164026   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-fc1a66e3750e1bf08110e815b6d552f5849fa5510f16f22ada5e011b6fb6a9a6-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164060   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-fc1a66e3750e1bf08110e815b6d552f5849fa5510f16f22ada5e011b6fb6a9a6-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164080   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-fc1a66e3750e1bf08110e815b6d552f5849fa5510f16f22ada5e011b6fb6a9a6-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164099   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-default.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164115   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-default.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164132   11716 manager.go:930] ignoring container "/system.slice/run-docker-netns-default.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164147   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-user-1000.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164163   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/run-user-1000.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164180   11716 manager.go:930] ignoring container "/system.slice/run-user-1000.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164195   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-98563e9e7629.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164211   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-98563e9e7629.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164228   11716 manager.go:930] ignoring container "/system.slice/run-docker-netns-98563e9e7629.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164244   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-29b24912\\x2daa4a\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7eempty\\x2ddir-collector\\x2dtextfiles.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164265   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-29b24912\\x2daa4a\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7eempty\\x2ddir-collector\\x2dtextfiles.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164286   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-29b24912\\x2daa4a\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7eempty\\x2ddir-collector\\x2dtextfiles.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164308   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-01996038e41a2ed7da881cec6f6f8ec935765b1ac294a698233b00536e7729ea-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164327   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-01996038e41a2ed7da881cec6f6f8ec935765b1ac294a698233b00536e7729ea-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164347   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-01996038e41a2ed7da881cec6f6f8ec935765b1ac294a698233b00536e7729ea-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164366   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-41c1e432a7c0d7045ed8c9af8bc2007187171305fa8eb8efaec35a921a9b786c-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164385   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-41c1e432a7c0d7045ed8c9af8bc2007187171305fa8eb8efaec35a921a9b786c-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164407   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-41c1e432a7c0d7045ed8c9af8bc2007187171305fa8eb8efaec35a921a9b786c-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164425   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-8f15ac20111a578c85ede93b0e9ed118eef0e47f1430274a512d8a421b949d6e-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164445   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-8f15ac20111a578c85ede93b0e9ed118eef0e47f1430274a512d8a421b949d6e-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164465   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-8f15ac20111a578c85ede93b0e9ed118eef0e47f1430274a512d8a421b949d6e-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164484   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-fdc459a06b542a82332062f240c64dd51fe7da5143de937ab31bdc315dad7613-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164503   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-fdc459a06b542a82332062f240c64dd51fe7da5143de937ab31bdc315dad7613-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164522   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-fdc459a06b542a82332062f240c64dd51fe7da5143de937ab31bdc315dad7613-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164541   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-ad71e4fdd5480b245c59f599a2ca145921ca2bc7d81ab02cab46ebc32187301c-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164560   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-ad71e4fdd5480b245c59f599a2ca145921ca2bc7d81ab02cab46ebc32187301c-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164580   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-ad71e4fdd5480b245c59f599a2ca145921ca2bc7d81ab02cab46ebc32187301c-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164598   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-069ed35ae3db52c440e112fc11c67048760953142293120ea85ee28a3dbf61bb-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164617   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-069ed35ae3db52c440e112fc11c67048760953142293120ea85ee28a3dbf61bb-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164637   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-069ed35ae3db52c440e112fc11c67048760953142293120ea85ee28a3dbf61bb-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164655   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-498f7dbdf3fecfb678d1cdf04f4df2303750f30592af40c85f940a75a0c02e51-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164674   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-498f7dbdf3fecfb678d1cdf04f4df2303750f30592af40c85f940a75a0c02e51-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164694   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-498f7dbdf3fecfb678d1cdf04f4df2303750f30592af40c85f940a75a0c02e51-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164712   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-7227bb31efcfbd11f3161fc6293a65a136e49d33ac0700b6a19254cfcdedd0d0-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164730   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-7227bb31efcfbd11f3161fc6293a65a136e49d33ac0700b6a19254cfcdedd0d0-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164750   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-7227bb31efcfbd11f3161fc6293a65a136e49d33ac0700b6a19254cfcdedd0d0-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164768   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-f67209c9733a4ec6b40e16054ee6fa7fcf8126b4595bca1843440cd2d1be6be0-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164787   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-f67209c9733a4ec6b40e16054ee6fa7fcf8126b4595bca1843440cd2d1be6be0-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164807   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-f67209c9733a4ec6b40e16054ee6fa7fcf8126b4595bca1843440cd2d1be6be0-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164826   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164845   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164865   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164884   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-cb67264bc72b7c34c5dce718b3266a82ec5a273e4bf127b50f0083b3736838f4-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164902   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-cb67264bc72b7c34c5dce718b3266a82ec5a273e4bf127b50f0083b3736838f4-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164922   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-cb67264bc72b7c34c5dce718b3266a82ec5a273e4bf127b50f0083b3736838f4-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164941   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-6ab4c5f55842.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164957   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-6ab4c5f55842.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164974   11716 manager.go:930] ignoring container "/system.slice/run-docker-netns-6ab4c5f55842.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.164989   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-0d3162f6893d788010883fd1407ae37262e31280dd4d45a823995dd8cdf0d2ef-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.165095   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-0d3162f6893d788010883fd1407ae37262e31280dd4d45a823995dd8cdf0d2ef-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.165120   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-0d3162f6893d788010883fd1407ae37262e31280dd4d45a823995dd8cdf0d2ef-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.165139   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-ce5470aa4deaa0a330bf26cdd6a90c701fc1ec5c18126e914dea5e6ef99177da-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.165158   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-ce5470aa4deaa0a330bf26cdd6a90c701fc1ec5c18126e914dea5e6ef99177da-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.165178   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-ce5470aa4deaa0a330bf26cdd6a90c701fc1ec5c18126e914dea5e6ef99177da-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.165196   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-kernel-debug.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.165211   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-kernel-debug.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169365   11716 manager.go:930] ignoring container "/system.slice/sys-kernel-debug.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169396   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-ba116b0570a921f2d70e3c1f57264f95517305da12ae5bb6b28f7c620b7b075a-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169423   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-ba116b0570a921f2d70e3c1f57264f95517305da12ae5bb6b28f7c620b7b075a-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169447   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-ba116b0570a921f2d70e3c1f57264f95517305da12ae5bb6b28f7c620b7b075a-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.167942   11716 secret.go:186] Setting up volume default-token-dbt8n for pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169468   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/-.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169603   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/-.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169624   11716 manager.go:930] ignoring container "/system.slice/-.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169642   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-mqueue.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169659   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-mqueue.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169678   11716 manager.go:930] ignoring container "/system.slice/dev-mqueue.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169730   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-2354195481aec80d4f6224d801fc51e73e9ff36031cfbf0a059ce187f635ef00-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169754   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-2354195481aec80d4f6224d801fc51e73e9ff36031cfbf0a059ce187f635ef00-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169777   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-2354195481aec80d4f6224d801fc51e73e9ff36031cfbf0a059ce187f635ef00-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169798   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/dev-hugepages.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169815   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/dev-hugepages.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169866   11716 manager.go:930] ignoring container "/system.slice/dev-hugepages.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169883   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-96cd6b417d5bee33989fbcf693b18647179b73388581c79bfb1d422f8b7cdf8d-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169905   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-96cd6b417d5bee33989fbcf693b18647179b73388581c79bfb1d422f8b7cdf8d-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169927   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-96cd6b417d5bee33989fbcf693b18647179b73388581c79bfb1d422f8b7cdf8d-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169948   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-2672003bf4007dadb6989fa4f3a9680efbcd1d1249c1c983a308d2f672764e6c-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169969   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-2672003bf4007dadb6989fa4f3a9680efbcd1d1249c1c983a308d2f672764e6c-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.169991   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-2672003bf4007dadb6989fa4f3a9680efbcd1d1249c1c983a308d2f672764e6c-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170012   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170088   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170112   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170154   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-360f774547db49fddb48c28d2d83c4bb700d219f4e5494ec0b14f18f5142e166-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170178   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-360f774547db49fddb48c28d2d83c4bb700d219f4e5494ec0b14f18f5142e166-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170201   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-360f774547db49fddb48c28d2d83c4bb700d219f4e5494ec0b14f18f5142e166-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170222   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-b4deca6afab46d84a1ac8bda1b63cde8391900632d906d8f79fed4f1204cd3a2-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170244   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-b4deca6afab46d84a1ac8bda1b63cde8391900632d906d8f79fed4f1204cd3a2-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170266   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-b4deca6afab46d84a1ac8bda1b63cde8391900632d906d8f79fed4f1204cd3a2-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170287   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-1bce3d7b35d061241c491af7a45a8a6e49cf919516622c7781c825c33296e993-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170308   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-1bce3d7b35d061241c491af7a45a8a6e49cf919516622c7781c825c33296e993-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170331   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-1bce3d7b35d061241c491af7a45a8a6e49cf919516622c7781c825c33296e993-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170352   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/sys-fs-fuse-connections.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170370   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/sys-fs-fuse-connections.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170389   11716 manager.go:930] ignoring container "/system.slice/sys-fs-fuse-connections.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170407   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-cb67264bc72b7c34c5dce718b3266a82ec5a273e4bf127b50f0083b3736838f4-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170429   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-cb67264bc72b7c34c5dce718b3266a82ec5a273e4bf127b50f0083b3736838f4-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170452   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-cb67264bc72b7c34c5dce718b3266a82ec5a273e4bf127b50f0083b3736838f4-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170473   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-d7a38ad227e92355e2045b27797b3a36afe48b60f9b1cb1f834ebb3f4f3de59f-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170494   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-d7a38ad227e92355e2045b27797b3a36afe48b60f9b1cb1f834ebb3f4f3de59f-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170517   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-d7a38ad227e92355e2045b27797b3a36afe48b60f9b1cb1f834ebb3f4f3de59f-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170538   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-eb6226bd06bc.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170556   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-eb6226bd06bc.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170575   11716 manager.go:930] ignoring container "/system.slice/run-docker-netns-eb6226bd06bc.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170593   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170615   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170637   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170658   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/run-docker-netns-e1be3e7ca7e6.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170676   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/run-docker-netns-e1be3e7ca7e6.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170696   11716 manager.go:930] ignoring container "/system.slice/run-docker-netns-e1be3e7ca7e6.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170714   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-d110bc793d810d0d4c3c5eb3c57710ee5e3f54b97a041cff9bc590eb7ab8c6ab-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170735   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-d110bc793d810d0d4c3c5eb3c57710ee5e3f54b97a041cff9bc590eb7ab8c6ab-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170758   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-d110bc793d810d0d4c3c5eb3c57710ee5e3f54b97a041cff9bc590eb7ab8c6ab-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170779   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-c8c5839ae6ba784b7201f796d61167d9b3b5ea3480aa8de80b09c8d5c6a2e224-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170800   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-c8c5839ae6ba784b7201f796d61167d9b3b5ea3480aa8de80b09c8d5c6a2e224-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170823   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-c8c5839ae6ba784b7201f796d61167d9b3b5ea3480aa8de80b09c8d5c6a2e224-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170848   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-8e41f9ca\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-kube\\x2dproxy\\x2dtoken\\x2d6hqvv.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170872   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-8e41f9ca\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-kube\\x2dproxy\\x2dtoken\\x2d6hqvv.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170933   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-8e41f9ca\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-kube\\x2dproxy\\x2dtoken\\x2d6hqvv.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170964   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-d110bc793d810d0d4c3c5eb3c57710ee5e3f54b97a041cff9bc590eb7ab8c6ab-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.170987   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-d110bc793d810d0d4c3c5eb3c57710ee5e3f54b97a041cff9bc590eb7ab8c6ab-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171011   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-d110bc793d810d0d4c3c5eb3c57710ee5e3f54b97a041cff9bc590eb7ab8c6ab-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171042   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-d53fefdf7557fdbedcce62abacdab2016e6b98a0e122b306779fa04622158664-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171066   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-d53fefdf7557fdbedcce62abacdab2016e6b98a0e122b306779fa04622158664-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171089   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-d53fefdf7557fdbedcce62abacdab2016e6b98a0e122b306779fa04622158664-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171110   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-efeaa58b46bbfef2205f44d0acf5b4bce51f4526c94b584cd76d9c95ac4e500f-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171132   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-efeaa58b46bbfef2205f44d0acf5b4bce51f4526c94b584cd76d9c95ac4e500f-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171155   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-efeaa58b46bbfef2205f44d0acf5b4bce51f4526c94b584cd76d9c95ac4e500f-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171176   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-f16a5de14d3fcbf89a173fd567ef9b90c408011110d5e667a4e547d66e14b019-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171197   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-f16a5de14d3fcbf89a173fd567ef9b90c408011110d5e667a4e547d66e14b019-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171245   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-f16a5de14d3fcbf89a173fd567ef9b90c408011110d5e667a4e547d66e14b019-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171268   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-d58b43f82a7fa456cd5acf1eeee0c71213dc4643d4dab9eae5bce8469c23897e-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171291   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-d58b43f82a7fa456cd5acf1eeee0c71213dc4643d4dab9eae5bce8469c23897e-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171314   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-d58b43f82a7fa456cd5acf1eeee0c71213dc4643d4dab9eae5bce8469c23897e-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171335   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-4d7b3de8474219cf09494fe8c30850972626fe4f01f4be7eec84c0357498ea30-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171356   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-4d7b3de8474219cf09494fe8c30850972626fe4f01f4be7eec84c0357498ea30-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171379   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-4d7b3de8474219cf09494fe8c30850972626fe4f01f4be7eec84c0357498ea30-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171400   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-e89869c79c9ed882bd60809d65f51caca2a1f9d97b5abd0e5fbdd8704f7388b1-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171422   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-e89869c79c9ed882bd60809d65f51caca2a1f9d97b5abd0e5fbdd8704f7388b1-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171445   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-e89869c79c9ed882bd60809d65f51caca2a1f9d97b5abd0e5fbdd8704f7388b1-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171466   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-95ec07a4\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d8flwv.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171490   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-95ec07a4\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d8flwv.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171514   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-95ec07a4\\x2daa35\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-default\\x2dtoken\\x2d8flwv.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171540   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-97afb50e72668aa81842377475658d061b0118378e92f275eaeae8a42d024267-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171563   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-97afb50e72668aa81842377475658d061b0118378e92f275eaeae8a42d024267-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171587   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-97afb50e72668aa81842377475658d061b0118378e92f275eaeae8a42d024267-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171608   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-fb4c19640bb587c27be309859e23479d4d556850075ba9d84194406e0e5bc9e4-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171629   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-fb4c19640bb587c27be309859e23479d4d556850075ba9d84194406e0e5bc9e4-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171652   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-fb4c19640bb587c27be309859e23479d4d556850075ba9d84194406e0e5bc9e4-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171673   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-ac9f59f702573f409e976a54e00d0579d8845784fcceecee7a31b14fe7196c4e-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171695   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-ac9f59f702573f409e976a54e00d0579d8845784fcceecee7a31b14fe7196c4e-mounts-shm.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171717   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-ac9f59f702573f409e976a54e00d0579d8845784fcceecee7a31b14fe7196c4e-mounts-shm.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171738   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-ac9f59f702573f409e976a54e00d0579d8845784fcceecee7a31b14fe7196c4e-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171760   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-ac9f59f702573f409e976a54e00d0579d8845784fcceecee7a31b14fe7196c4e-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171782   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-ac9f59f702573f409e976a54e00d0579d8845784fcceecee7a31b14fe7196c4e-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171803   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171824   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171847   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171868   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-baba1a72c949b39a9054c301f20c6d3bcac5a65e9dfeee5225039f38d9706dff-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171889   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-baba1a72c949b39a9054c301f20c6d3bcac5a65e9dfeee5225039f38d9706dff-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171912   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-baba1a72c949b39a9054c301f20c6d3bcac5a65e9dfeee5225039f38d9706dff-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171933   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-a17b3d80e006968192b94ec2ebd2d69824e88b4933185e55c6e55c61c51338ac-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171955   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-a17b3d80e006968192b94ec2ebd2d69824e88b4933185e55c6e55c61c51338ac-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171977   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-a17b3d80e006968192b94ec2ebd2d69824e88b4933185e55c6e55c61c51338ac-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.171998   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-0f58257bdc04e7331f270f8123d948141b976916ccc86a7133868f691fb3100d-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172020   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-0f58257bdc04e7331f270f8123d948141b976916ccc86a7133868f691fb3100d-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172052   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-0f58257bdc04e7331f270f8123d948141b976916ccc86a7133868f691fb3100d-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172075   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-kubelet-pods-4b593dd5\\x2daa36\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-tiller\\x2dtoken\\x2dnzvkl.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172098   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-kubelet-pods-4b593dd5\\x2daa36\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-tiller\\x2dtoken\\x2dnzvkl.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172124   11716 manager.go:930] ignoring container "/system.slice/var-lib-kubelet-pods-4b593dd5\\x2daa36\\x2d11e8\\x2da2ec\\x2dfa163e7aa991-volumes-kubernetes.io\\x7esecret-tiller\\x2dtoken\\x2dnzvkl.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172150   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-containers-d7a38ad227e92355e2045b27797b3a36afe48b60f9b1cb1f834ebb3f4f3de59f-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172172   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-containers-d7a38ad227e92355e2045b27797b3a36afe48b60f9b1cb1f834ebb3f4f3de59f-mounts.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172192   11716 secret.go:216] Received secret default/default-token-dbt8n containing (3) pieces of data, 1878 total bytes
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172195   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-containers-d7a38ad227e92355e2045b27797b3a36afe48b60f9b1cb1f834ebb3f4f3de59f-mounts.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172367   11716 factory.go:116] Factory "docker" was unable to handle container "/system.slice/var-lib-docker-overlay-bc4241c5d27fe117ccd9df1c7953ec963ab04ce6a6c94e1efce31402612fb3f9-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172391   11716 factory.go:109] Factory "systemd" can handle container "/system.slice/var-lib-docker-overlay-bc4241c5d27fe117ccd9df1c7953ec963ab04ce6a6c94e1efce31402612fb3f9-merged.mount", but ignoring.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172415   11716 manager.go:930] ignoring container "/system.slice/var-lib-docker-overlay-bc4241c5d27fe117ccd9df1c7953ec963ab04ce6a6c94e1efce31402612fb3f9-merged.mount"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172438   11716 manager.go:391] Global Housekeeping(1535412682) took 219.054778ms
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.172575   11716 atomic_writer.go:156] pod default/node-exporter-node-8z55w volume default-token-dbt8n: no update required for target directory /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.441826   11716 volume_manager.go:372] All volumes are attached and mounted for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.442203   11716 kuberuntime_manager.go:514] Container {Name:nvidia-dcgm-exporter Image:containerdevk8s/dcgm-exporter:1.4.3 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:collector-textfiles ReadOnly:false MountPath:/run/prometheus SubPath: MountPropagation:<nil>} {Name:default-token-dbt8n ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false ExtendedResourceRequests:[] ComputeResourceRequests:[]} is dead, but RestartPolicy says that we should restart it.
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.442242   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e4457e539de698350ee03306991d2d3f618821e047b72516692b6353aa256e7f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[1] ContainersToKill:map[]} for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.442454   11716 kuberuntime_manager.go:758] checking backoff for container "nvidia-dcgm-exporter" in pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.442617   11716 kuberuntime_manager.go:768] Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.442646   11716 kuberuntime_manager.go:721] Backing Off restarting container &Container{Name:nvidia-dcgm-exporter,Image:containerdevk8s/dcgm-exporter:1.4.3,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[{collector-textfiles false /run/prometheus  <nil>} {default-token-dbt8n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,VolumeDevices:[],ExtendedResourceRequests:[],ComputeResourceRequests:[],} in pod node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:23 swatig-vm kubelet[11716]: E0827 23:31:23.442769   11716 pod_workers.go:186] Error syncing pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 ("node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"), skipping: failed to "StartContainer" for "nvidia-dcgm-exporter" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.442856   11716 server.go:231] Event(v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"node-exporter-node-8z55w", UID:"9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991", APIVersion:"v1", ResourceVersion:"14310", FieldPath:"spec.containers{nvidia-dcgm-exporter}", ExtendedResources:v1.ExtendedResourceBinding(nil)}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
Aug 27 23:31:23 swatig-vm kubelet[11716]: I0827 23:31:23.604663   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.133537   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.296160   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.296221   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.308951   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Content-Type:[text/plain; charset=utf-8] Content-Length:[2] Date:[Mon, 27 Aug 2018 23:31:24 GMT]] 0xc4219e64e0 2 [] false false map[] 0xc420ba4d00 0xc42199a8f0}
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.309064   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.778498   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.877267   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49446264Ki, capacity: 61847136Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.877327   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528696Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.877350   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.877368   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528696Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.877386   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:09.328438303 +0000 UTC m=+1249.735947534
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.877403   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61125144Ki, capacity: 61847136Ki
Aug 27 23:31:24 swatig-vm kubelet[11716]: I0827 23:31:24.877434   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:31:26 swatig-vm kubelet[11716]: I0827 23:31:26.133395   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:27 swatig-vm kubelet[11716]: I0827 23:31:27.143684   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:31:27 swatig-vm kubelet[11716]: I0827 23:31:27.143753   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:27 swatig-vm kubelet[11716]: I0827 23:31:27.144946   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:27 GMT] Content-Length:[0]] 0xc4219a57a0 0 [] true false map[] 0xc421f78d00 <nil>}
Aug 27 23:31:27 swatig-vm kubelet[11716]: I0827 23:31:27.145019   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:28 swatig-vm kubelet[11716]: I0827 23:31:28.133537   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:28 swatig-vm kubelet[11716]: I0827 23:31:28.607134   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:29 swatig-vm kubelet[11716]: I0827 23:31:29.357766   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:31:29 swatig-vm kubelet[11716]: I0827 23:31:29.357833   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:29 swatig-vm kubelet[11716]: I0827 23:31:29.359488   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:29 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc42108dfe0 2 [] true false map[] 0xc420998100 <nil>}
Aug 27 23:31:29 swatig-vm kubelet[11716]: I0827 23:31:29.359556   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.133587   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.150729   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.150782   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.152064   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:30 GMT] Content-Length:[0]] 0xc4229a0000 0 [] true false map[] 0xc422a5b700 <nil>}
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.152151   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.416118   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.416174   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.417528   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:30 GMT] Content-Length:[50]] 0xc4210c65c0 50 [] true false map[] 0xc421f79200 <nil>}
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.417592   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.723223   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.723293   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.724630   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[3] Content-Type:[text/plain; charset=utf-8] Date:[Mon, 27 Aug 2018 23:31:30 GMT]] 0xc420b74460 3 [] true false map[] 0xc422496200 <nil>}
Aug 27 23:31:30 swatig-vm kubelet[11716]: I0827 23:31:30.724710   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:31 swatig-vm kubelet[11716]: I0827 23:31:31.539780   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:31:31 swatig-vm kubelet[11716]: I0827 23:31:31.539839   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:31 swatig-vm kubelet[11716]: I0827 23:31:31.541451   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:31 GMT] Content-Length:[18] Content-Type:[text/plain; charset=utf-8]] 0xc422c0ede0 18 [] true false map[] 0xc422496900 <nil>}
Aug 27 23:31:31 swatig-vm kubelet[11716]: I0827 23:31:31.541524   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.133488   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.478721   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.478772   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.479821   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:32 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc421b274a0 2 [] true false map[] 0xc4221b4400 <nil>}
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.479891   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.547730   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.547770   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.548269   11716 interface.go:174] Interface ens3 is up
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.548431   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.548467   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.548487   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.548504   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.548521   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.548568   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.885769   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.885818   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.895979   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:31:32 GMT]] 0xc421b27a20 -1 [] true true map[] 0xc420fe3500 <nil>}
Aug 27 23:31:32 swatig-vm kubelet[11716]: I0827 23:31:32.896092   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:31:33 swatig-vm kubelet[11716]: I0827 23:31:33.018286   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:31:33 swatig-vm kubelet[11716]: I0827 23:31:33.018344   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:33 swatig-vm kubelet[11716]: I0827 23:31:33.019483   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[51] Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:33 GMT]] 0xc422337480 51 [] true false map[] 0xc420fe3700 <nil>}
Aug 27 23:31:33 swatig-vm kubelet[11716]: I0827 23:31:33.019548   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:31:33 swatig-vm kubelet[11716]: I0827 23:31:33.609334   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.133548   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.296180   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.297141   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.309858   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Content-Type:[text/plain; charset=utf-8] Content-Length:[2] Date:[Mon, 27 Aug 2018 23:31:34 GMT]] 0xc4220204a0 2 [] false false map[] 0xc422a5af00 0xc420ff29a0}
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.309995   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.877749   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.970917   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49445228Ki, capacity: 61847136Ki, time: 2018-08-27 23:31:24.974466554 +0000 UTC m=+1265.381975772
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.971512   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528596Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:24.974466554 +0000 UTC m=+1265.381975772
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.972035   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:24.974466554 +0000 UTC m=+1265.381975772
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.972569   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528596Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:24.974466554 +0000 UTC m=+1265.381975772
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.973097   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:24.974466554 +0000 UTC m=+1265.381975772
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.973659   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61124772Ki, capacity: 61847136Ki
Aug 27 23:31:34 swatig-vm kubelet[11716]: I0827 23:31:34.974167   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.133452   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.133546   11716 kubelet_pods.go:1382] Generating status for "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.133959   11716 status_manager.go:353] Ignoring same status for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:28:59 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [nvidia-dcgm-exporter]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:05 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 23:12:04 +0000 UTC InitContainerStatuses:[{Name:nvidia-dcgm-exporter-hook State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:12:06 +0000 UTC,FinishedAt:2018-08-27 23:12:06 +0000 UTC,ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e}] ContainerStatuses:[{Name:node-exporter State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 23:12:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/prometheus/node-exporter:v0.15.2 ImageID:docker-pullable://quay.io/prometheus/node-exporter@sha256:0c7dd2350bed76fce17dff8bd2a2ac599bc989c7328eb77b0751b8024cf0457d ContainerID:docker://76b7f83d228f08c90324183faf81862e43af14282064503023d73c07af58b035} {Name:nvidia-dcgm-exporter State:{Waiting:&ContainerStateWaiting{Reason:CrashLoo
Aug 27 23:31:35 swatig-vm kubelet[11716]: pBackOff,Message:Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:28:53 +0000 UTC,FinishedAt:2018-08-27 23:28:58 +0000 UTC,ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840,}} Ready:false RestartCount:8 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840}] QOSClass:Burstable}
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.134479   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.213152   11716 secret.go:186] Setting up volume default-token-dbt8n for pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.216397   11716 secret.go:216] Received secret default/default-token-dbt8n containing (3) pieces of data, 1878 total bytes
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.216841   11716 atomic_writer.go:156] pod default/node-exporter-node-8z55w volume default-token-dbt8n: no update required for target directory /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.434937   11716 volume_manager.go:372] All volumes are attached and mounted for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.435448   11716 kuberuntime_manager.go:514] Container {Name:nvidia-dcgm-exporter Image:containerdevk8s/dcgm-exporter:1.4.3 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:collector-textfiles ReadOnly:false MountPath:/run/prometheus SubPath: MountPropagation:<nil>} {Name:default-token-dbt8n ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false ExtendedResourceRequests:[] ComputeResourceRequests:[]} is dead, but RestartPolicy says that we should restart it.
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.435516   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e4457e539de698350ee03306991d2d3f618821e047b72516692b6353aa256e7f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[1] ContainersToKill:map[]} for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.435833   11716 kuberuntime_manager.go:758] checking backoff for container "nvidia-dcgm-exporter" in pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.436129   11716 kuberuntime_manager.go:768] Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.436192   11716 kuberuntime_manager.go:721] Backing Off restarting container &Container{Name:nvidia-dcgm-exporter,Image:containerdevk8s/dcgm-exporter:1.4.3,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[{collector-textfiles false /run/prometheus  <nil>} {default-token-dbt8n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,VolumeDevices:[],ExtendedResourceRequests:[],ComputeResourceRequests:[],} in pod node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:35 swatig-vm kubelet[11716]: E0827 23:31:35.436392   11716 pod_workers.go:186] Error syncing pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 ("node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"), skipping: failed to "StartContainer" for "nvidia-dcgm-exporter" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:35 swatig-vm kubelet[11716]: I0827 23:31:35.436353   11716 server.go:231] Event(v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"node-exporter-node-8z55w", UID:"9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991", APIVersion:"v1", ResourceVersion:"14310", FieldPath:"spec.containers{nvidia-dcgm-exporter}", ExtendedResources:v1.ExtendedResourceBinding(nil)}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.133557   11716 kubelet.go:1921] SyncLoop (SYNC): 2 pods; kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991), kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6)
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.133675   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.133731   11716 kubelet_pods.go:1382] Generating status for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.133912   11716 kubelet_pods.go:1382] Generating status for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6)"
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.134089   11716 status_manager.go:353] Ignoring same status for pod "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:10:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:14 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:10.244.0.5 StartTime:2018-08-27 20:13:14 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:dnsmasq State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:13:22 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/k8s-dns-dnsmasq-nanny-amd64:1.14.7 ImageID:docker-pullable://containerdevk8s/k8s-dns-dnsmasq-nanny-amd64@sha256:1a9d4ab13b51d2b132ea633e62a9a6333236127935e2c6b8363254004dc45139 ContainerID:docker://2354195481aec80d4f6224d801fc51e73e9ff36031cfbf0a059ce187f635ef00} {Name:kubedns State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:13:19 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/k8s-dns-kube-dns-amd64:1.14.7 ImageID:docker-pullable://containerdevk8s/k8s-dns-kube-dns-amd64@sha256:9bbb3d16ded224349190f0d1a160154fac1bae7bcdad4293f041da9a7d7344e9 ContainerID:docker://d58b43f82a7fa456cd5acf1eeee0c71213dc4643d4dab9eae5bce8469c23897e} {Name:sidecar State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:13:25 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/k8s-dns-sidecar-amd64:1.14.7 ImageID:docker-pullable://containerdevk8s/k8s-dns-
Aug 27 23:31:36 swatig-vm kubelet[11716]: sidecar-amd64@sha256:d9776c3a3e0d777bfe9a12d8b45d18ee7e7f31c31597440dcd4c1b15d391d247 ContainerID:docker://97afb50e72668aa81842377475658d061b0118378e92f275eaeae8a42d024267}] QOSClass:Burstable}
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.134605   11716 status_manager.go:353] Ignoring same status for pod "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:10:23 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:12 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-scheduler State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:13 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/kube-scheduler-amd64:v1.9.10 ImageID:docker-pullable://containerdevk8s/kube-scheduler-amd64@sha256:00c0b7157169183bfe8e6966c4d18b8a6dc1946b8b0f3e34970fecceea819da5 ContainerID:docker://8f15ac20111a578c85ede93b0e9ed118eef0e47f1430274a512d8a421b949d6e}] QOSClass:Burstable}
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.134895   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.135031   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6)"
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.217215   11716 secret.go:186] Setting up volume kube-dns-token-dmjrw for pod 8e3a46b3-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e3a46b3-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/kube-dns-token-dmjrw
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.217315   11716 configmap.go:187] Setting up volume kube-dns-config for pod 8e3a46b3-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e3a46b3-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/kube-dns-config
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.219686   11716 secret.go:216] Received secret kube-system/kube-dns-token-dmjrw containing (3) pieces of data, 1896 total bytes
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.220507   11716 atomic_writer.go:156] pod kube-system/kube-dns-55856cb6b6-5tr5q volume kube-dns-token-dmjrw: no update required for target directory /var/lib/kubelet/pods/8e3a46b3-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/kube-dns-token-dmjrw
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.220635   11716 configmap.go:217] Received configMap kube-system/kube-dns containing (0) pieces of data, 0 total bytes
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.220795   11716 atomic_writer.go:156] pod kube-system/kube-dns-55856cb6b6-5tr5q volume kube-dns-config: no update required for target directory /var/lib/kubelet/pods/8e3a46b3-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/kube-dns-config
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.435372   11716 volume_manager.go:372] All volumes are attached and mounted for pod "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6)"
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.435376   11716 volume_manager.go:372] All volumes are attached and mounted for pod "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.435591   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:0d3162f6893d788010883fd1407ae37262e31280dd4d45a823995dd8cdf0d2ef Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6)"
Aug 27 23:31:36 swatig-vm kubelet[11716]: I0827 23:31:36.435973   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:ac9f59f702573f409e976a54e00d0579d8845784fcceecee7a31b14fe7196c4e Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:37 swatig-vm kubelet[11716]: I0827 23:31:37.143673   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:31:37 swatig-vm kubelet[11716]: I0827 23:31:37.143738   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:37 swatig-vm kubelet[11716]: I0827 23:31:37.145171   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:37 GMT] Content-Length:[0]] 0xc421970020 0 [] true false map[] 0xc4221b5000 <nil>}
Aug 27 23:31:37 swatig-vm kubelet[11716]: I0827 23:31:37.145293   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:38 swatig-vm kubelet[11716]: I0827 23:31:38.133444   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:38 swatig-vm kubelet[11716]: I0827 23:31:38.611620   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:39 swatig-vm kubelet[11716]: I0827 23:31:39.357782   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:31:39 swatig-vm kubelet[11716]: I0827 23:31:39.357845   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:39 swatig-vm kubelet[11716]: I0827 23:31:39.359250   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:39 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc422cd7e60 2 [] true false map[] 0xc420ba5900 <nil>}
Aug 27 23:31:39 swatig-vm kubelet[11716]: I0827 23:31:39.359322   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.133517   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.150647   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.151612   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.153466   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:40 GMT] Content-Length:[0]] 0xc422ca2640 0 [] true false map[] 0xc421590100 <nil>}
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.153532   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.416043   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.416123   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.417403   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:40 GMT] Content-Length:[51] Content-Type:[application/json]] 0xc422cd7d40 51 [] true false map[] 0xc42046e800 <nil>}
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.417465   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.723184   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.723234   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.724448   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:40 GMT] Content-Length:[3] Content-Type:[text/plain; charset=utf-8]] 0xc422cd7ec0 3 [] true false map[] 0xc42046eb00 <nil>}
Aug 27 23:31:40 swatig-vm kubelet[11716]: I0827 23:31:40.724517   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:41 swatig-vm kubelet[11716]: I0827 23:31:41.539801   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:31:41 swatig-vm kubelet[11716]: I0827 23:31:41.539871   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:41 swatig-vm kubelet[11716]: I0827 23:31:41.541477   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; charset=utf-8] Date:[Mon, 27 Aug 2018 23:31:41 GMT] Content-Length:[18]] 0xc422091680 18 [] true false map[] 0xc42144ef00 <nil>}
Aug 27 23:31:41 swatig-vm kubelet[11716]: I0827 23:31:41.542416   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.133541   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.478744   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.479662   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.482013   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:42 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc4201e78a0 2 [] true false map[] 0xc420999800 <nil>}
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.482976   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.565188   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.566033   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.567267   11716 interface.go:174] Interface ens3 is up
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.568263   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.569106   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.569951   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.570721   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.571501   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.572268   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.885841   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.885900   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.897794   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:31:42 GMT]] 0xc422249600 -1 [] true true map[] 0xc421590e00 <nil>}
Aug 27 23:31:42 swatig-vm kubelet[11716]: I0827 23:31:42.897866   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.018458   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.018516   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.019642   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:43 GMT] Content-Length:[51]] 0xc4222496e0 51 [] true false map[] 0xc420e93900 <nil>}
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.019716   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.133522   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29)
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.133626   11716 kubelet_pods.go:1382] Generating status for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29)"
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.133879   11716 status_manager.go:353] Ignoring same status for pod "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:10:23 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:12 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:13 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/kube-apiserver-amd64:v1.9.10 ImageID:docker-pullable://containerdevk8s/kube-apiserver-amd64@sha256:f4563ec4f187c669460a09a84ec9a4c352b777d9f8bbf21fe63d4773a8d1fc72 ContainerID:docker://01996038e41a2ed7da881cec6f6f8ec935765b1ac294a698233b00536e7729ea}] QOSClass:Burstable}
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.134263   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29)"
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.434631   11716 volume_manager.go:372] All volumes are attached and mounted for pod "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29)"
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.434886   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:f16a5de14d3fcbf89a173fd567ef9b90c408011110d5e667a4e547d66e14b019 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29)"
Aug 27 23:31:43 swatig-vm kubelet[11716]: I0827 23:31:43.614009   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:44 swatig-vm kubelet[11716]: I0827 23:31:44.133499   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:44 swatig-vm kubelet[11716]: I0827 23:31:44.296188   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:31:44 swatig-vm kubelet[11716]: I0827 23:31:44.296299   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:44 swatig-vm kubelet[11716]: I0827 23:31:44.308655   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Content-Type:[text/plain; charset=utf-8] Content-Length:[2] Date:[Mon, 27 Aug 2018 23:31:44 GMT]] 0xc4226feba0 2 [] false false map[] 0xc421591200 0xc42199bad0}
Aug 27 23:31:44 swatig-vm kubelet[11716]: I0827 23:31:44.308741   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:31:44 swatig-vm kubelet[11716]: I0827 23:31:44.974963   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.065078   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528564Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.065136   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.065159   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61124856Ki, capacity: 61847136Ki
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.065174   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49445164Ki, capacity: 61847136Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.065191   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528564Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.065208   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.065294   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.133501   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; kube-flannel-ds-tzbqf_kube-system(8e425969-aa35-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.133609   11716 kubelet_pods.go:1382] Generating status for "kube-flannel-ds-tzbqf_kube-system(8e425969-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.133957   11716 status_manager.go:353] Ignoring same status for pod "kube-flannel-ds-tzbqf_kube-system(8e425969-aa35-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:54 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:43 +0000 UTC InitContainerStatuses:[{Name:install-cni State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 20:12:53 +0000 UTC,FinishedAt:2018-08-27 20:12:53 +0000 UTC,ContainerID:docker://aeb2ac97461ae366207c12554049bd24d6de3b0b0cac7e8e83d4a05001e31370,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/coreos/flannel:v0.9.1-amd64 ImageID:docker-pullable://quay.io/coreos/flannel@sha256:056cf57fd3bbe7264c0be1a3b34ec2e289b33e51c70f332f4e88aa83970ad891 ContainerID:docker://aeb2ac97461ae366207c12554049bd24d6de3b0b0cac7e8e83d4a05001e31370}] ContainerStatuses:[{Name:kube-flannel State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:54 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/coreos/flannel:v0.9.1-amd64 ImageID:docker-pullable://quay.io/coreos/flannel@sha256:056cf57fd3bbe7264c0be1a3b34ec2e289b33e51c70f332f4e88aa83970ad891 ContainerID:docker://7227bb31efcfbd11f3161fc6293a65a136e49d33ac0700b6a19254cfcdedd0d0}] QOSClass:BestEffort}
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.134374   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "kube-flannel-ds-tzbqf_kube-system(8e425969-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.148924   11716 configmap.go:187] Setting up volume flannel-cfg for pod 8e425969-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e425969-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/flannel-cfg
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.148977   11716 secret.go:186] Setting up volume flannel-token-dww48 for pod 8e425969-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e425969-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/flannel-token-dww48
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.151388   11716 configmap.go:217] Received configMap kube-system/kube-flannel-cfg containing (2) pieces of data, 165 total bytes
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.151462   11716 secret.go:216] Received secret kube-system/flannel-token-dww48 containing (3) pieces of data, 1892 total bytes
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.151713   11716 atomic_writer.go:156] pod kube-system/kube-flannel-ds-tzbqf volume flannel-cfg: no update required for target directory /var/lib/kubelet/pods/8e425969-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/flannel-cfg
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.151867   11716 atomic_writer.go:156] pod kube-system/kube-flannel-ds-tzbqf volume flannel-token-dww48: no update required for target directory /var/lib/kubelet/pods/8e425969-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/flannel-token-dww48
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.434863   11716 volume_manager.go:372] All volumes are attached and mounted for pod "kube-flannel-ds-tzbqf_kube-system(8e425969-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:45 swatig-vm kubelet[11716]: I0827 23:31:45.435154   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:d7a38ad227e92355e2045b27797b3a36afe48b60f9b1cb1f834ebb3f4f3de59f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "kube-flannel-ds-tzbqf_kube-system(8e425969-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.133462   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.133530   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.133720   11716 kubelet_pods.go:1382] Generating status for "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.134093   11716 status_manager.go:353] Ignoring same status for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:28:59 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [nvidia-dcgm-exporter]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:05 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 23:12:04 +0000 UTC InitContainerStatuses:[{Name:nvidia-dcgm-exporter-hook State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:12:06 +0000 UTC,FinishedAt:2018-08-27 23:12:06 +0000 UTC,ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e}] ContainerStatuses:[{Name:node-exporter State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 23:12:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/prometheus/node-exporter:v0.15.2 ImageID:docker-pullable://quay.io/prometheus/node-exporter@sha256:0c7dd2350bed76fce17dff8bd2a2ac599bc989c7328eb77b0751b8024cf0457d ContainerID:docker://76b7f83d228f08c90324183faf81862e43af14282064503023d73c07af58b035} {Name:nvidia-dcgm-exporter State:{Waiting:&ContainerStateWaiting{Reason:CrashLoo
Aug 27 23:31:46 swatig-vm kubelet[11716]: pBackOff,Message:Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:28:53 +0000 UTC,FinishedAt:2018-08-27 23:28:58 +0000 UTC,ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840,}} Ready:false RestartCount:8 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840}] QOSClass:Burstable}
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.134495   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.152746   11716 secret.go:186] Setting up volume default-token-dbt8n for pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.155199   11716 secret.go:216] Received secret default/default-token-dbt8n containing (3) pieces of data, 1878 total bytes
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.155528   11716 atomic_writer.go:156] pod default/node-exporter-node-8z55w volume default-token-dbt8n: no update required for target directory /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.435085   11716 volume_manager.go:372] All volumes are attached and mounted for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.435555   11716 kuberuntime_manager.go:514] Container {Name:nvidia-dcgm-exporter Image:containerdevk8s/dcgm-exporter:1.4.3 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:collector-textfiles ReadOnly:false MountPath:/run/prometheus SubPath: MountPropagation:<nil>} {Name:default-token-dbt8n ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false ExtendedResourceRequests:[] ComputeResourceRequests:[]} is dead, but RestartPolicy says that we should restart it.
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.435622   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e4457e539de698350ee03306991d2d3f618821e047b72516692b6353aa256e7f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[1] ContainersToKill:map[]} for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.435934   11716 kuberuntime_manager.go:758] checking backoff for container "nvidia-dcgm-exporter" in pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.436210   11716 kuberuntime_manager.go:768] Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.436236   11716 kuberuntime_manager.go:721] Backing Off restarting container &Container{Name:nvidia-dcgm-exporter,Image:containerdevk8s/dcgm-exporter:1.4.3,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[{collector-textfiles false /run/prometheus  <nil>} {default-token-dbt8n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,VolumeDevices:[],ExtendedResourceRequests:[],ComputeResourceRequests:[],} in pod node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:46 swatig-vm kubelet[11716]: I0827 23:31:46.436358   11716 server.go:231] Event(v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"node-exporter-node-8z55w", UID:"9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991", APIVersion:"v1", ResourceVersion:"14310", FieldPath:"spec.containers{nvidia-dcgm-exporter}", ExtendedResources:v1.ExtendedResourceBinding(nil)}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
Aug 27 23:31:46 swatig-vm kubelet[11716]: E0827 23:31:46.436426   11716 pod_workers.go:186] Error syncing pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 ("node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"), skipping: failed to "StartContainer" for "nvidia-dcgm-exporter" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:47 swatig-vm kubelet[11716]: I0827 23:31:47.143787   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:31:47 swatig-vm kubelet[11716]: I0827 23:31:47.143846   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:47 swatig-vm kubelet[11716]: I0827 23:31:47.145364   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:47 GMT] Content-Length:[0]] 0xc422f18780 0 [] true false map[] 0xc421591a00 <nil>}
Aug 27 23:31:47 swatig-vm kubelet[11716]: I0827 23:31:47.145434   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.133552   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25)
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.133635   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.133837   11716 kubelet_pods.go:1382] Generating status for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25)"
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.134188   11716 status_manager.go:353] Ignoring same status for pod "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:10:23 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:12 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-controller-manager State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:13 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/kube-controller-manager-amd64:v1.9.10 ImageID:docker-pullable://containerdevk8s/kube-controller-manager-amd64@sha256:4540e9be098b898b315dff70beec25baebd77e6f1fbdb2d2d932f126f14c3701 ContainerID:docker://b4deca6afab46d84a1ac8bda1b63cde8391900632d906d8f79fed4f1204cd3a2}] QOSClass:Burstable}
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.134605   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25)"
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.435119   11716 volume_manager.go:372] All volumes are attached and mounted for pod "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25)"
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.435369   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:cb67264bc72b7c34c5dce718b3266a82ec5a273e4bf127b50f0083b3736838f4 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25)"
Aug 27 23:31:48 swatig-vm kubelet[11716]: I0827 23:31:48.616319   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:49 swatig-vm kubelet[11716]: I0827 23:31:49.357731   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:31:49 swatig-vm kubelet[11716]: I0827 23:31:49.357789   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:49 swatig-vm kubelet[11716]: I0827 23:31:49.359233   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:49 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc422630100 2 [] true false map[] 0xc420ba4600 <nil>}
Aug 27 23:31:49 swatig-vm kubelet[11716]: I0827 23:31:49.359376   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.133493   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.150595   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.150637   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.151934   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:50 GMT] Content-Length:[0]] 0xc4226305a0 0 [] true false map[] 0xc422496700 <nil>}
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.151988   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.415942   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.416011   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.417148   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:50 GMT] Content-Length:[51]] 0xc422630dc0 51 [] true false map[] 0xc4221b5200 <nil>}
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.417251   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.723048   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.723122   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.724367   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:50 GMT] Content-Length:[3] Content-Type:[text/plain; charset=utf-8]] 0xc420ee09e0 3 [] true false map[] 0xc4221b5b00 <nil>}
Aug 27 23:31:50 swatig-vm kubelet[11716]: I0827 23:31:50.724422   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:31:51 swatig-vm kubelet[11716]: I0827 23:31:51.539722   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:31:51 swatig-vm kubelet[11716]: I0827 23:31:51.539781   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:51 swatig-vm kubelet[11716]: I0827 23:31:51.541509   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:51 GMT] Content-Length:[18] Content-Type:[text/plain; charset=utf-8]] 0xc422f07d00 18 [] true false map[] 0xc420ba4a00 <nil>}
Aug 27 23:31:51 swatig-vm kubelet[11716]: I0827 23:31:51.541576   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.133441   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.478770   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.478823   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.479991   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:52 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc42037c9c0 2 [] true false map[] 0xc4221b5e00 <nil>}
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.480057   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.587505   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.587546   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.588015   11716 interface.go:174] Interface ens3 is up
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.588205   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.588243   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.588264   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.588283   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.588302   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.588351   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.885783   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.885838   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.895267   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:31:52 GMT]] 0xc420fe5fc0 -1 [] true true map[] 0xc420fe2a00 <nil>}
Aug 27 23:31:52 swatig-vm kubelet[11716]: I0827 23:31:52.895327   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:31:53 swatig-vm kubelet[11716]: I0827 23:31:53.018403   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:31:53 swatig-vm kubelet[11716]: I0827 23:31:53.018456   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:53 swatig-vm kubelet[11716]: I0827 23:31:53.019972   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:31:53 GMT] Content-Length:[51]] 0xc422091180 51 [] true false map[] 0xc420e92c00 <nil>}
Aug 27 23:31:53 swatig-vm kubelet[11716]: I0827 23:31:53.020034   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:31:53 swatig-vm kubelet[11716]: I0827 23:31:53.618478   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.133485   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; prometheus-operator-789d76f7b9-jfqll_monitoring(a4e6f975-aa36-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.133561   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.133686   11716 kubelet_pods.go:1382] Generating status for "prometheus-operator-789d76f7b9-jfqll_monitoring(a4e6f975-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.133956   11716 status_manager.go:353] Ignoring same status for pod "prometheus-operator-789d76f7b9-jfqll_monitoring(a4e6f975-aa36-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:20:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:20:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:20:30 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:10.244.0.8 StartTime:2018-08-27 20:20:30 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:prometheus-operator State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:20:55 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/coreos/prometheus-operator:v0.17.0 ImageID:docker-pullable://quay.io/coreos/prometheus-operator@sha256:02d980dbbb513b7d0b063132a1bd0c7aa41abe8171289f910568098268b8d215 ContainerID:docker://4ebb3f9dbc89b8ca0e9365c7d3466d4c3e89af3988f90cc48ae14f20435a88d3}] QOSClass:BestEffort}
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.134370   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "prometheus-operator-789d76f7b9-jfqll_monitoring(a4e6f975-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.181362   11716 secret.go:186] Setting up volume prometheus-operator-token-r2g9q for pod a4e6f975-aa36-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/a4e6f975-aa36-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/prometheus-operator-token-r2g9q
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.183698   11716 secret.go:216] Received secret monitoring/prometheus-operator-token-r2g9q containing (3) pieces of data, 1937 total bytes
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.184111   11716 atomic_writer.go:156] pod monitoring/prometheus-operator-789d76f7b9-jfqll volume prometheus-operator-token-r2g9q: no update required for target directory /var/lib/kubelet/pods/a4e6f975-aa36-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/prometheus-operator-token-r2g9q
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.296121   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.296166   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.308029   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Content-Type:[text/plain; charset=utf-8] Content-Length:[2] Date:[Mon, 27 Aug 2018 23:31:54 GMT]] 0xc420eef420 2 [] false false map[] 0xc420e93200 0xc42185b550}
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.308132   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.434743   11716 volume_manager.go:372] All volumes are attached and mounted for pod "prometheus-operator-789d76f7b9-jfqll_monitoring(a4e6f975-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:54 swatig-vm kubelet[11716]: I0827 23:31:54.434874   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:d110bc793d810d0d4c3c5eb3c57710ee5e3f54b97a041cff9bc590eb7ab8c6ab Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "prometheus-operator-789d76f7b9-jfqll_monitoring(a4e6f975-aa36-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.065626   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.155981   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528564Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.156037   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.156059   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61124740Ki, capacity: 61847136Ki
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.156085   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49445164Ki, capacity: 61847136Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.156102   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528564Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.156117   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:42.184404616 +0000 UTC m=+1282.591913865
Aug 27 23:31:55 swatig-vm kubelet[11716]: I0827 23:31:55.156147   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.133524   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.133605   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.133752   11716 kubelet_pods.go:1382] Generating status for "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.133997   11716 status_manager.go:353] Ignoring same status for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:49 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:43 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:kube-proxy State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:49 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/kube-proxy-amd64:v1.9.10 ImageID:docker-pullable://containerdevk8s/kube-proxy-amd64@sha256:ea3aef50e4b9d17616af8beb339da298c0ab71569c816c6628edfff4bdd0908a ContainerID:docker://d53fefdf7557fdbedcce62abacdab2016e6b98a0e122b306779fa04622158664}] QOSClass:BestEffort}
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.134377   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.188101   11716 configmap.go:187] Setting up volume kube-proxy for pod 8e41f9ca-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/kube-proxy
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.188117   11716 secret.go:186] Setting up volume kube-proxy-token-6hqvv for pod 8e41f9ca-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/kube-proxy-token-6hqvv
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.190703   11716 secret.go:216] Received secret kube-system/kube-proxy-token-6hqvv containing (3) pieces of data, 1904 total bytes
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.190710   11716 configmap.go:217] Received configMap kube-system/kube-proxy containing (2) pieces of data, 1209 total bytes
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.191155   11716 atomic_writer.go:156] pod kube-system/kube-proxy-x6vpl volume kube-proxy-token-6hqvv: no update required for target directory /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/kube-proxy-token-6hqvv
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.191253   11716 atomic_writer.go:156] pod kube-system/kube-proxy-x6vpl volume kube-proxy: no update required for target directory /var/lib/kubelet/pods/8e41f9ca-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~configmap/kube-proxy
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.434818   11716 volume_manager.go:372] All volumes are attached and mounted for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:56 swatig-vm kubelet[11716]: I0827 23:31:56.434999   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:343a68828f811739328359c272481f14a9c18411911d1e63c71a33bf4f0629bb Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "kube-proxy-x6vpl_kube-system(8e41f9ca-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.133492   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.133611   11716 kubelet_pods.go:1382] Generating status for "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.133962   11716 status_manager.go:353] Ignoring same status for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:06 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:28:59 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [nvidia-dcgm-exporter]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:12:05 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 23:12:04 +0000 UTC InitContainerStatuses:[{Name:nvidia-dcgm-exporter-hook State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:12:06 +0000 UTC,FinishedAt:2018-08-27 23:12:06 +0000 UTC,ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://fe6118a58cfa583eea15a899b580e5951fa4ae2b104f09ce600a8db0b9f4351e}] ContainerStatuses:[{Name:node-exporter State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 23:12:07 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:quay.io/prometheus/node-exporter:v0.15.2 ImageID:docker-pullable://quay.io/prometheus/node-exporter@sha256:0c7dd2350bed76fce17dff8bd2a2ac599bc989c7328eb77b0751b8024cf0457d ContainerID:docker://76b7f83d228f08c90324183faf81862e43af14282064503023d73c07af58b035} {Name:nvidia-dcgm-exporter State:{Waiting:&ContainerStateWaiting{Reason:CrashLoo
Aug 27 23:31:57 swatig-vm kubelet[11716]: pBackOff,Message:Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991),} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 23:28:53 +0000 UTC,FinishedAt:2018-08-27 23:28:58 +0000 UTC,ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840,}} Ready:false RestartCount:8 Image:containerdevk8s/dcgm-exporter:1.4.3 ImageID:docker-pullable://containerdevk8s/dcgm-exporter@sha256:fe924a7529c5bd533f9c364128f4882c54b73637370edc20dd77f9cfc2dc796f ContainerID:docker://06f09dddef22cadc57411e299e41d483d4e4ab74c0aed7d7510c046b7df47840}] QOSClass:Burstable}
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.134432   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.143756   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.143818   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.145049   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:57 GMT] Content-Length:[0]] 0xc421a86520 0 [] true false map[] 0xc420e93200 <nil>}
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.145126   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.191875   11716 secret.go:186] Setting up volume default-token-dbt8n for pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.194344   11716 secret.go:216] Received secret default/default-token-dbt8n containing (3) pieces of data, 1878 total bytes
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.194713   11716 atomic_writer.go:156] pod default/node-exporter-node-8z55w volume default-token-dbt8n: no update required for target directory /var/lib/kubelet/pods/9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-dbt8n
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.434840   11716 volume_manager.go:372] All volumes are attached and mounted for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.435140   11716 kuberuntime_manager.go:514] Container {Name:nvidia-dcgm-exporter Image:containerdevk8s/dcgm-exporter:1.4.3 Command:[] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:collector-textfiles ReadOnly:false MountPath:/run/prometheus SubPath: MountPropagation:<nil>} {Name:default-token-dbt8n ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false ExtendedResourceRequests:[] ComputeResourceRequests:[]} is dead, but RestartPolicy says that we should restart it.
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.435176   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:e4457e539de698350ee03306991d2d3f618821e047b72516692b6353aa256e7f Attempt:0 NextInitContainerToStart:nil ContainersToStart:[1] ContainersToKill:map[]} for pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.435376   11716 kuberuntime_manager.go:758] checking backoff for container "nvidia-dcgm-exporter" in pod "node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.435550   11716 kuberuntime_manager.go:768] Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.435577   11716 kuberuntime_manager.go:721] Backing Off restarting container &Container{Name:nvidia-dcgm-exporter,Image:containerdevk8s/dcgm-exporter:1.4.3,Command:[],Args:[],WorkingDir:,Ports:[],Env:[],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[{collector-textfiles false /run/prometheus  <nil>} {default-token-dbt8n true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}],LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*0,RunAsNonRoot:*false,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[],TerminationMessagePolicy:File,VolumeDevices:[],ExtendedResourceRequests:[],ComputeResourceRequests:[],} in pod node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)
Aug 27 23:31:57 swatig-vm kubelet[11716]: E0827 23:31:57.435696   11716 pod_workers.go:186] Error syncing pod 9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991 ("node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"), skipping: failed to "StartContainer" for "nvidia-dcgm-exporter" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=nvidia-dcgm-exporter pod=node-exporter-node-8z55w_default(9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991)"
Aug 27 23:31:57 swatig-vm kubelet[11716]: I0827 23:31:57.435786   11716 server.go:231] Event(v1.ObjectReference{Kind:"Pod", Namespace:"default", Name:"node-exporter-node-8z55w", UID:"9ccde6fc-aa4e-11e8-a2ec-fa163e7aa991", APIVersion:"v1", ResourceVersion:"14310", FieldPath:"spec.containers{nvidia-dcgm-exporter}", ExtendedResources:v1.ExtendedResourceBinding(nil)}): type: 'Warning' reason: 'BackOff' Back-off restarting failed container
Aug 27 23:31:58 swatig-vm kubelet[11716]: I0827 23:31:58.133486   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:31:58 swatig-vm kubelet[11716]: I0827 23:31:58.620946   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:31:59 swatig-vm kubelet[11716]: I0827 23:31:59.357715   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10252, Path: /healthz
Aug 27 23:31:59 swatig-vm kubelet[11716]: I0827 23:31:59.357779   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:31:59 swatig-vm kubelet[11716]: I0827 23:31:59.359485   11716 http.go:96] Probe succeeded for http://127.0.0.1:10252/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:31:59 GMT] Content-Length:[2] Content-Type:[text/plain; charset=utf-8]] 0xc420f73b80 2 [] true false map[] 0xc420ba4d00 <nil>}
Aug 27 23:31:59 swatig-vm kubelet[11716]: I0827 23:31:59.359570   11716 prober.go:118] Liveness probe for "kube-controller-manager-swatig-vm_kube-system(644f2a3788514dd332a909a5a1401c25):kube-controller-manager" succeeded
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.133426   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.150625   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /readiness
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.150662   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.151453   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:32:00 GMT] Content-Length:[0]] 0xc420f469c0 0 [] true false map[] 0xc420b9d900 <nil>}
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.151494   11716 prober.go:118] Readiness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.415981   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/kubedns
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.416044   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.417445   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/kubedns, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:32:00 GMT] Content-Length:[51]] 0xc422021f00 51 [] true false map[] 0xc420b9db00 <nil>}
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.417519   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.723032   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 8081, Path: /readiness
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.723106   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.724548   11716 http.go:96] Probe succeeded for http://10.244.0.5:8081/readiness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:32:00 GMT] Content-Length:[3] Content-Type:[text/plain; charset=utf-8]] 0xc422f06ce0 3 [] true false map[] 0xc42046ee00 <nil>}
Aug 27 23:32:00 swatig-vm kubelet[11716]: I0827 23:32:00.724622   11716 prober.go:118] Readiness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):kubedns" succeeded
Aug 27 23:32:01 swatig-vm kubelet[11716]: I0827 23:32:01.539729   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 2379, Path: /health
Aug 27 23:32:01 swatig-vm kubelet[11716]: I0827 23:32:01.539800   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:01 swatig-vm kubelet[11716]: I0827 23:32:01.541616   11716 http.go:96] Probe succeeded for http://127.0.0.1:2379/health, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:32:01 GMT] Content-Length:[18] Content-Type:[text/plain; charset=utf-8]] 0xc421a73e20 18 [] true false map[] 0xc420ba5600 <nil>}
Aug 27 23:32:01 swatig-vm kubelet[11716]: I0827 23:32:01.541731   11716 prober.go:118] Liveness probe for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4):etcd" succeeded
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.133562   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.478727   11716 prober.go:165] HTTP-Probe Host: http://127.0.0.1, Port: 10251, Path: /healthz
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.478826   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.480102   11716 http.go:96] Probe succeeded for http://127.0.0.1:10251/healthz, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Length:[2] Content-Type:[text/plain; charset=utf-8] Date:[Mon, 27 Aug 2018 23:32:02 GMT]] 0xc421b27540 2 [] true false map[] 0xc421f78200 <nil>}
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.480176   11716 prober.go:118] Liveness probe for "kube-scheduler-swatig-vm_kube-system(aa9edd700f3907547b591b3bfe063cd6):kube-scheduler" succeeded
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604158   11716 interface.go:360] Looking for default routes with IPv4 addresses
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604195   11716 interface.go:365] Default route transits interface "ens3"
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604539   11716 interface.go:174] Interface ens3 is up
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604664   11716 interface.go:222] Interface "ens3" has 2 addresses :[192.168.102.19/24 fe80::f816:3eff:fe7a:a991/64].
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604691   11716 interface.go:189] Checking addr  192.168.102.19/24.
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604706   11716 interface.go:196] IP found 192.168.102.19
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604719   11716 interface.go:228] Found valid IPv4 address 192.168.102.19 for interface "ens3".
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604731   11716 interface.go:371] Found active IP 192.168.102.19
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.604770   11716 kubelet_node_status.go:615] Update capacity for nvidia.com/gpu to 1
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.885848   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /metrics
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.885902   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.895478   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/metrics, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[text/plain; version=0.0.4] Date:[Mon, 27 Aug 2018 23:32:02 GMT]] 0xc421e75a40 -1 [] true true map[] 0xc421f78f00 <nil>}
Aug 27 23:32:02 swatig-vm kubelet[11716]: I0827 23:32:02.895572   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):sidecar" succeeded
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.018389   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.5, Port: 10054, Path: /healthcheck/dnsmasq
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.018442   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.019684   11716 http.go:96] Probe succeeded for http://10.244.0.5:10054/healthcheck/dnsmasq, Response: {200 OK 200 HTTP/1.1 1 1 map[Content-Type:[application/json] Date:[Mon, 27 Aug 2018 23:32:03 GMT] Content-Length:[51]] 0xc422336fc0 51 [] true false map[] 0xc421591f00 <nil>}
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.019748   11716 prober.go:118] Liveness probe for "kube-dns-55856cb6b6-5tr5q_kube-system(8e3a46b3-aa35-11e8-a2ec-fa163e7aa991):dnsmasq" succeeded
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.133518   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.133630   11716 kubelet_pods.go:1382] Generating status for "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.133977   11716 status_manager.go:353] Ignoring same status for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:13:08 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:10.244.0.4 StartTime:2018-08-27 20:12:56 +0000 UTC InitContainerStatuses:[{Name:nvidia-device-plugin-install-hooks State:{Waiting:nil Running:nil Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2018-08-27 20:13:16 +0000 UTC,FinishedAt:2018-08-27 20:13:17 +0000 UTC,ContainerID:docker://7995e2f592845dadc2ab7b5fe764b95833f0b8fd967fc77ade4bbdcc25544f8e,}} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/device-plugin:1.9.10 ImageID:docker-pullable://containerdevk8s/device-plugin@sha256:81de32888c89a1371ce585d2d35f71284b91d99deef7f34a963e2ba13a674b31 ContainerID:docker://7995e2f592845dadc2ab7b5fe764b95833f0b8fd967fc77ade4bbdcc25544f8e}] ContainerStatuses:[{Name:nvidia-device-plugin-ctr State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:13:18 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/device-plugin:1.9.10 ImageID:docker-pullable://containerdevk8s/device-plugin@sha256:81de32888c89a1371ce585d2d35f71284b91d99deef7f34a963e2ba13a674b31 ContainerID:docker://360f774547db49fddb48c28d2d83c4bb700d219f4e5494ec0b14f18f5142e166}] QOSClass:BestEffort}
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.134393   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.212826   11716 secret.go:186] Setting up volume default-token-8flwv for pod 95ec07a4-aa35-11e8-a2ec-fa163e7aa991 at /var/lib/kubelet/pods/95ec07a4-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-8flwv
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.215008   11716 secret.go:216] Received secret kube-system/default-token-8flwv containing (3) pieces of data, 1892 total bytes
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.215432   11716 atomic_writer.go:156] pod kube-system/nvidia-device-plugin-daemonset-6fbqg volume default-token-8flwv: no update required for target directory /var/lib/kubelet/pods/95ec07a4-aa35-11e8-a2ec-fa163e7aa991/volumes/kubernetes.io~secret/default-token-8flwv
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.434857   11716 volume_manager.go:372] All volumes are attached and mounted for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.435018   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:380eb934a6b27666fde0653ee3929a5a006d3426475bc96248c3dff7e432a15a Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "nvidia-device-plugin-daemonset-6fbqg_kube-system(95ec07a4-aa35-11e8-a2ec-fa163e7aa991)"
Aug 27 23:32:03 swatig-vm kubelet[11716]: I0827 23:32:03.623398   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
Aug 27 23:32:04 swatig-vm kubelet[11716]: I0827 23:32:04.133388   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:32:04 swatig-vm kubelet[11716]: I0827 23:32:04.296165   11716 prober.go:165] HTTP-Probe Host: https://192.168.102.19, Port: 6443, Path: /healthz
Aug 27 23:32:04 swatig-vm kubelet[11716]: I0827 23:32:04.296221   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:04 swatig-vm kubelet[11716]: I0827 23:32:04.308755   11716 http.go:96] Probe succeeded for https://192.168.102.19:6443/healthz, Response: {200 OK 200 HTTP/2.0 2 0 map[Date:[Mon, 27 Aug 2018 23:32:04 GMT] Content-Type:[text/plain; charset=utf-8] Content-Length:[2]] 0xc4228e4480 2 [] false false map[] 0xc42144f300 0xc422aabce0}
Aug 27 23:32:04 swatig-vm kubelet[11716]: I0827 23:32:04.308843   11716 prober.go:118] Liveness probe for "kube-apiserver-swatig-vm_kube-system(e30c09ec53cab3c5f7ab59d391b3cb29):kube-apiserver" succeeded
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.156403   11716 eviction_manager.go:221] eviction manager: synchronize housekeeping
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.241640   11716 helpers.go:827] eviction manager: observations: signal=nodefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:58.308501637 +0000 UTC m=+1298.716010995
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.241704   11716 helpers.go:827] eviction manager: observations: signal=imagefs.available, available: 8528512Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:58.308501637 +0000 UTC m=+1298.716010995
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.241726   11716 helpers.go:827] eviction manager: observations: signal=imagefs.inodesFree, available: 3997082, capacity: 5000Ki, time: 2018-08-27 23:31:58.308501637 +0000 UTC m=+1298.716010995
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.241743   11716 helpers.go:829] eviction manager: observations: signal=allocatableMemory.available, available: 61124604Ki, capacity: 61847136Ki
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.241757   11716 helpers.go:827] eviction manager: observations: signal=memory.available, available: 49444464Ki, capacity: 61847136Ki, time: 2018-08-27 23:31:58.308501637 +0000 UTC m=+1298.716010995
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.241773   11716 helpers.go:827] eviction manager: observations: signal=nodefs.available, available: 8528512Ki, capacity: 40593708Ki, time: 2018-08-27 23:31:58.308501637 +0000 UTC m=+1298.716010995
Aug 27 23:32:05 swatig-vm kubelet[11716]: I0827 23:32:05.241803   11716 eviction_manager.go:325] eviction manager: no resources are starved
Aug 27 23:32:06 swatig-vm kubelet[11716]: I0827 23:32:06.133585   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.133463   11716 kubelet.go:1921] SyncLoop (SYNC): 1 pods; etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.133632   11716 kubelet_pods.go:1382] Generating status for "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.133931   11716 status_manager.go:353] Ignoring same status for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)", status: {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 20:12:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2018-08-27 23:10:23 +0000 UTC Reason: Message:}] Message: Reason: HostIP:192.168.102.19 PodIP:192.168.102.19 StartTime:2018-08-27 20:12:12 +0000 UTC InitContainerStatuses:[] ContainerStatuses:[{Name:etcd State:{Waiting:nil Running:&ContainerStateRunning{StartedAt:2018-08-27 20:12:13 +0000 UTC,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:containerdevk8s/etcd-amd64:3.1.11 ImageID:docker-pullable://containerdevk8s/etcd-amd64@sha256:cc14b8e827085f97117e118618ef817b562b91abe1ca8e363357fe68f68311e3 ContainerID:docker://18a9649a6e568bff7f6ab37928f4d563623fe180a2ff0eb860aaa391f4332ef3}] QOSClass:BestEffort}
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.134301   11716 volume_manager.go:343] Waiting for volumes to attach and mount for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.143674   11716 prober.go:165] HTTP-Probe Host: http://10.244.0.6, Port: 44135, Path: /liveness
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.143728   11716 prober.go:168] HTTP-Probe Headers: map[]
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.145022   11716 http.go:96] Probe succeeded for http://10.244.0.6:44135/liveness, Response: {200 OK 200 HTTP/1.1 1 1 map[Date:[Mon, 27 Aug 2018 23:32:07 GMT] Content-Length:[0]] 0xc421e74440 0 [] true false map[] 0xc4215a5400 <nil>}
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.145131   11716 prober.go:118] Liveness probe for "tiller-deploy-79d7d4fddd-22q4w_kube-system(4b593dd5-aa36-11e8-a2ec-fa163e7aa991):tiller" succeeded
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.434730   11716 volume_manager.go:372] All volumes are attached and mounted for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:32:07 swatig-vm kubelet[11716]: I0827 23:32:07.434918   11716 kuberuntime_manager.go:571] computePodActions got {KillPod:false CreateSandbox:false SandboxID:c4c14b58671a4a148c40e23d1c86dcadef766171a10d8df9925435a263c8a568 Attempt:0 NextInitContainerToStart:nil ContainersToStart:[] ContainersToKill:map[]} for pod "etcd-swatig-vm_kube-system(4376aa1f28af9c9ffdde5deed212e0a4)"
Aug 27 23:32:08 swatig-vm kubelet[11716]: I0827 23:32:08.133561   11716 kubelet.go:1944] SyncLoop (housekeeping)
Aug 27 23:32:08 swatig-vm kubelet[11716]: I0827 23:32:08.625910   11716 kubelet.go:2123] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:
